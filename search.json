[
  {
    "objectID": "reference/llm_summarize.html",
    "href": "reference/llm_summarize.html",
    "title": "Summarize text",
    "section": "",
    "text": "R/llm-summarize.R"
  },
  {
    "objectID": "reference/llm_summarize.html#llm_summarize",
    "href": "reference/llm_summarize.html#llm_summarize",
    "title": "Summarize text",
    "section": "llm_summarize",
    "text": "llm_summarize"
  },
  {
    "objectID": "reference/llm_summarize.html#description",
    "href": "reference/llm_summarize.html#description",
    "title": "Summarize text",
    "section": "Description",
    "text": "Description\nUse a Large Language Model (LLM) to summarize text"
  },
  {
    "objectID": "reference/llm_summarize.html#usage",
    "href": "reference/llm_summarize.html#usage",
    "title": "Summarize text",
    "section": "Usage",
    "text": "Usage\n \nllm_summarize( \n  .data, \n  col, \n  max_words = 10, \n  pred_name = \".summary\", \n  additional_prompt = \"\" \n) \n \nllm_vec_summarize(x, max_words = 10, additional_prompt = \"\", preview = FALSE)"
  },
  {
    "objectID": "reference/llm_summarize.html#arguments",
    "href": "reference/llm_summarize.html#arguments",
    "title": "Summarize text",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\n.data\nA data.frame or tbl object that contains the text to be analyzed\n\n\ncol\nThe name of the field to analyze, supports tidy-eval\n\n\nmax_words\nThe maximum number of words that the LLM should use in the summary. Defaults to 10.\n\n\npred_name\nA character vector with the name of the new column where the prediction will be placed\n\n\nadditional_prompt\nInserts this text into the prompt sent to the LLM\n\n\nx\nA vector that contains the text to be analyzed\n\n\npreview\nIt returns the R call that would have been used to run the prediction. It only returns the first record in x. Defaults to FALSE Applies to vector function only."
  },
  {
    "objectID": "reference/llm_summarize.html#value",
    "href": "reference/llm_summarize.html#value",
    "title": "Summarize text",
    "section": "Value",
    "text": "Value\nllm_summarize returns a data.frame or tbl object. llm_vec_summarize returns a vector that is the same length as x."
  },
  {
    "objectID": "reference/llm_summarize.html#examples",
    "href": "reference/llm_summarize.html#examples",
    "title": "Summarize text",
    "section": "Examples",
    "text": "Examples\n\n \nlibrary(mall) \n \ndata(\"reviews\") \n \nllm_use(\"ollama\", \"llama3.2\", seed = 100, .silent = TRUE) \n \n# Use max_words to set the maximum number of words to use for the summary \nllm_summarize(reviews, review, max_words = 5) \n#&gt; # A tibble: 3 × 2\n#&gt;   review                                        .summary                      \n#&gt;   &lt;chr&gt;                                         &lt;chr&gt;                         \n#&gt; 1 This has been the best TV I've ever used. Gr… it's a great tv               \n#&gt; 2 I regret buying this laptop. It is too slow … laptop purchase was a mistake \n#&gt; 3 Not sure how to feel about my new washing ma… having mixed feelings about it\n \n# Use 'pred_name' to customize the new column's name \nllm_summarize(reviews, review, 5, pred_name = \"review_summary\") \n#&gt; # A tibble: 3 × 2\n#&gt;   review                                        review_summary                \n#&gt;   &lt;chr&gt;                                         &lt;chr&gt;                         \n#&gt; 1 This has been the best TV I've ever used. Gr… it's a great tv               \n#&gt; 2 I regret buying this laptop. It is too slow … laptop purchase was a mistake \n#&gt; 3 Not sure how to feel about my new washing ma… having mixed feelings about it\n \n# For character vectors, instead of a data frame, use this function \nllm_vec_summarize( \n  \"This has been the best TV I've ever used. Great screen, and sound.\", \n  max_words = 5 \n) \n#&gt; [1] \"it's a great tv\"\n \n# To preview the first call that will be made to the downstream R function \nllm_vec_summarize( \n  \"This has been the best TV I've ever used. Great screen, and sound.\", \n  max_words = 5, \n  preview = TRUE \n) \n#&gt; ollamar::chat(messages = list(list(role = \"user\", content = \"You are a helpful summarization engine. Your answer will contain no no capitalization and no explanations. Return no more than 5 words.  The answer is the summary of the following text:\\nThis has been the best TV I've ever used. Great screen, and sound.\")), \n#&gt;     output = \"text\", model = \"llama3.2\", seed = 100)"
  },
  {
    "objectID": "reference/m_backend_submit.html",
    "href": "reference/m_backend_submit.html",
    "title": "Functions to integrate different back-ends",
    "section": "",
    "text": "R/m-backend-prompt.R, R/m-backend-submit.R"
  },
  {
    "objectID": "reference/m_backend_submit.html#m_backend_prompt",
    "href": "reference/m_backend_submit.html#m_backend_prompt",
    "title": "Functions to integrate different back-ends",
    "section": "m_backend_prompt",
    "text": "m_backend_prompt"
  },
  {
    "objectID": "reference/m_backend_submit.html#description",
    "href": "reference/m_backend_submit.html#description",
    "title": "Functions to integrate different back-ends",
    "section": "Description",
    "text": "Description\nFunctions to integrate different back-ends"
  },
  {
    "objectID": "reference/m_backend_submit.html#usage",
    "href": "reference/m_backend_submit.html#usage",
    "title": "Functions to integrate different back-ends",
    "section": "Usage",
    "text": "Usage\n \nm_backend_prompt(backend, additional) \n \nm_backend_submit(backend, x, prompt, preview = FALSE)"
  },
  {
    "objectID": "reference/m_backend_submit.html#arguments",
    "href": "reference/m_backend_submit.html#arguments",
    "title": "Functions to integrate different back-ends",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nbackend\nAn mall_session object\n\n\nadditional\nAdditional text to insert to the base_prompt\n\n\nx\nThe body of the text to be submitted to the LLM\n\n\nprompt\nThe additional information to add to the submission\n\n\npreview\nIf TRUE, it will display the resulting R call of the first text in x"
  },
  {
    "objectID": "reference/m_backend_submit.html#value",
    "href": "reference/m_backend_submit.html#value",
    "title": "Functions to integrate different back-ends",
    "section": "Value",
    "text": "Value\nm_backend_submit does not return an object. m_backend_prompt returns a list of functions that contain the base prompts."
  },
  {
    "objectID": "reference/llm_use.html",
    "href": "reference/llm_use.html",
    "title": "Specify the model to use",
    "section": "",
    "text": "R/llm-use.R"
  },
  {
    "objectID": "reference/llm_use.html#llm_use",
    "href": "reference/llm_use.html#llm_use",
    "title": "Specify the model to use",
    "section": "llm_use",
    "text": "llm_use"
  },
  {
    "objectID": "reference/llm_use.html#description",
    "href": "reference/llm_use.html#description",
    "title": "Specify the model to use",
    "section": "Description",
    "text": "Description\nAllows us to specify the back-end provider, model to use during the current R session"
  },
  {
    "objectID": "reference/llm_use.html#usage",
    "href": "reference/llm_use.html#usage",
    "title": "Specify the model to use",
    "section": "Usage",
    "text": "Usage\n \nllm_use( \n  backend = NULL, \n  model = NULL, \n  ..., \n  .silent = FALSE, \n  .cache = NULL, \n  .force = FALSE \n)"
  },
  {
    "objectID": "reference/llm_use.html#arguments",
    "href": "reference/llm_use.html#arguments",
    "title": "Specify the model to use",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nbackend\nThe name of an supported back-end provider. Currently only ‘ollama’ is supported.\n\n\nmodel\nThe name of model supported by the back-end provider\n\n\n…\nAdditional arguments that this function will pass down to the integrating function. In the case of Ollama, it will pass those arguments to ollamar::chat().\n\n\n.silent\nAvoids console output\n\n\n.cache\nThe path to save model results, so they can be re-used if the same operation is ran again. To turn off, set this argument to an empty character: \"\". ‘It defaults to’_mall_cache’. If this argument is left NULL when calling this function, no changes to the path will be made.\n\n\n.force\nFlag that tell the function to reset all of the settings in the R session"
  },
  {
    "objectID": "reference/llm_use.html#value",
    "href": "reference/llm_use.html#value",
    "title": "Specify the model to use",
    "section": "Value",
    "text": "Value\nA mall_session object"
  },
  {
    "objectID": "reference/llm_use.html#examples",
    "href": "reference/llm_use.html#examples",
    "title": "Specify the model to use",
    "section": "Examples",
    "text": "Examples\n\n \nlibrary(mall) \n \nllm_use(\"ollama\", \"llama3.2\") \n#&gt; \n#&gt; ── mall session object\n#&gt; Backend: ollama\n#&gt; LLM session: model:llama3.2\n#&gt; R session: cache_folder:_mall_cache\n \n# Additional arguments will be passed 'as-is' to the \n# downstream R function in this example, to ollama::chat() \nllm_use(\"ollama\", \"llama3.2\", seed = 100, temp = 0.1) \n#&gt; \n#&gt; ── mall session object \n#&gt; Backend: ollamaLLM session:  model:llama3.2\n#&gt;   seed:100\n#&gt;   temp:0.1\n#&gt; R session: cache_folder:_mall_cache\n \n# During the R session, you can change any argument \n# individually and it will retain all of previous \n# arguments used \nllm_use(temp = 0.3) \n#&gt; \n#&gt; ── mall session object \n#&gt; Backend: ollamaLLM session:  model:llama3.2\n#&gt;   seed:100\n#&gt;   temp:0.3\n#&gt; R session: cache_folder:_mall_cache\n \n# Use .cache to modify the target folder for caching \nllm_use(.cache = \"_my_cache\") \n#&gt; \n#&gt; ── mall session object \n#&gt; Backend: ollamaLLM session:  model:llama3.2\n#&gt;   seed:100\n#&gt;   temp:0.3\n#&gt; R session: cache_folder:_my_cache\n \n# Leave .cache empty to turn off this functionality \nllm_use(.cache = \"\") \n#&gt; \n#&gt; ── mall session object \n#&gt; Backend: ollamaLLM session:  model:llama3.2\n#&gt;   seed:100\n#&gt;   temp:0.3\n \n# Use .silent to avoid the print out \nllm_use(.silent = TRUE)"
  },
  {
    "objectID": "reference/reviews.html",
    "href": "reference/reviews.html",
    "title": "Mini reviews data set",
    "section": "",
    "text": "R/data-reviews.R"
  },
  {
    "objectID": "reference/reviews.html#reviews",
    "href": "reference/reviews.html#reviews",
    "title": "Mini reviews data set",
    "section": "reviews",
    "text": "reviews"
  },
  {
    "objectID": "reference/reviews.html#description",
    "href": "reference/reviews.html#description",
    "title": "Mini reviews data set",
    "section": "Description",
    "text": "Description\nMini reviews data set"
  },
  {
    "objectID": "reference/reviews.html#format",
    "href": "reference/reviews.html#format",
    "title": "Mini reviews data set",
    "section": "Format",
    "text": "Format\nA data frame that contains 3 records. The records are of fictitious product reviews."
  },
  {
    "objectID": "reference/reviews.html#usage",
    "href": "reference/reviews.html#usage",
    "title": "Mini reviews data set",
    "section": "Usage",
    "text": "Usage\n \nreviews"
  },
  {
    "objectID": "reference/reviews.html#examples",
    "href": "reference/reviews.html#examples",
    "title": "Mini reviews data set",
    "section": "Examples",
    "text": "Examples\n\n \nlibrary(mall) \ndata(reviews) \nreviews \n#&gt; # A tibble: 3 × 1\n#&gt;   review                                                                        \n#&gt;   &lt;chr&gt;                                                                         \n#&gt; 1 This has been the best TV I've ever used. Great screen, and sound.            \n#&gt; 2 I regret buying this laptop. It is too slow and the keyboard is too noisy     \n#&gt; 3 Not sure how to feel about my new washing machine. Great color, but hard to f…"
  },
  {
    "objectID": "reference/llm_classify.html",
    "href": "reference/llm_classify.html",
    "title": "Categorize data as one of options given",
    "section": "",
    "text": "R/llm-classify.R"
  },
  {
    "objectID": "reference/llm_classify.html#llm_classify",
    "href": "reference/llm_classify.html#llm_classify",
    "title": "Categorize data as one of options given",
    "section": "llm_classify",
    "text": "llm_classify"
  },
  {
    "objectID": "reference/llm_classify.html#description",
    "href": "reference/llm_classify.html#description",
    "title": "Categorize data as one of options given",
    "section": "Description",
    "text": "Description\nUse a Large Language Model (LLM) to classify the provided text as one of the options provided via the labels argument."
  },
  {
    "objectID": "reference/llm_classify.html#usage",
    "href": "reference/llm_classify.html#usage",
    "title": "Categorize data as one of options given",
    "section": "Usage",
    "text": "Usage\n \nllm_classify( \n  .data, \n  col, \n  labels, \n  pred_name = \".classify\", \n  additional_prompt = \"\" \n) \n \nllm_vec_classify(x, labels, additional_prompt = \"\", preview = FALSE)"
  },
  {
    "objectID": "reference/llm_classify.html#arguments",
    "href": "reference/llm_classify.html#arguments",
    "title": "Categorize data as one of options given",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\n.data\nA data.frame or tbl object that contains the text to be analyzed\n\n\ncol\nThe name of the field to analyze, supports tidy-eval\n\n\nlabels\nA character vector with at least 2 labels to classify the text as\n\n\npred_name\nA character vector with the name of the new column where the prediction will be placed\n\n\nadditional_prompt\nInserts this text into the prompt sent to the LLM\n\n\nx\nA vector that contains the text to be analyzed\n\n\npreview\nIt returns the R call that would have been used to run the prediction. It only returns the first record in x. Defaults to FALSE Applies to vector function only."
  },
  {
    "objectID": "reference/llm_classify.html#value",
    "href": "reference/llm_classify.html#value",
    "title": "Categorize data as one of options given",
    "section": "Value",
    "text": "Value\nllm_classify returns a data.frame or tbl object. llm_vec_classify returns a vector that is the same length as x."
  },
  {
    "objectID": "reference/llm_classify.html#examples",
    "href": "reference/llm_classify.html#examples",
    "title": "Categorize data as one of options given",
    "section": "Examples",
    "text": "Examples\n\n \nlibrary(mall) \n \ndata(\"reviews\") \n \nllm_use(\"ollama\", \"llama3.2\", seed = 100, .silent = TRUE) \n \nllm_classify(reviews, review, c(\"appliance\", \"computer\")) \n#&gt; # A tibble: 3 × 2\n#&gt;   review                                        .classify\n#&gt;   &lt;chr&gt;                                         &lt;chr&gt;    \n#&gt; 1 This has been the best TV I've ever used. Gr… computer \n#&gt; 2 I regret buying this laptop. It is too slow … computer \n#&gt; 3 Not sure how to feel about my new washing ma… appliance\n \n# Use 'pred_name' to customize the new column's name \nllm_classify( \n  reviews, \n  review, \n  c(\"appliance\", \"computer\"), \n  pred_name = \"prod_type\" \n) \n#&gt; # A tibble: 3 × 2\n#&gt;   review                                        prod_type\n#&gt;   &lt;chr&gt;                                         &lt;chr&gt;    \n#&gt; 1 This has been the best TV I've ever used. Gr… computer \n#&gt; 2 I regret buying this laptop. It is too slow … computer \n#&gt; 3 Not sure how to feel about my new washing ma… appliance\n \n# Pass custom values for each classification \nllm_classify(reviews, review, c(\"appliance\" ~ 1, \"computer\" ~ 2)) \n#&gt; # A tibble: 3 × 2\n#&gt;   review                                                               .classify\n#&gt;   &lt;chr&gt;                                                                    &lt;dbl&gt;\n#&gt; 1 This has been the best TV I've ever used. Great screen, and sound.           1\n#&gt; 2 I regret buying this laptop. It is too slow and the keyboard is too…         2\n#&gt; 3 Not sure how to feel about my new washing machine. Great color, but…         1\n \n# For character vectors, instead of a data frame, use this function \nllm_vec_classify( \n  c(\"this is important!\", \"just whenever\"), \n  c(\"urgent\", \"not urgent\") \n) \n#&gt; [1] \"urgent\" \"urgent\"\n \n# To preview the first call that will be made to the downstream R function \nllm_vec_classify( \n  c(\"this is important!\", \"just whenever\"), \n  c(\"urgent\", \"not urgent\"), \n  preview = TRUE \n) \n#&gt; ollamar::chat(messages = list(list(role = \"user\", content = \"You are a helpful classification engine. Determine if the text refers to one of the following: urgent, not urgent. No capitalization. No explanations.  The answer is based on the following text:\\nthis is important!\")), \n#&gt;     output = \"text\", model = \"llama3.2\", seed = 100)"
  },
  {
    "objectID": "reference/llm_sentiment.html",
    "href": "reference/llm_sentiment.html",
    "title": "Sentiment analysis",
    "section": "",
    "text": "R/llm-sentiment.R"
  },
  {
    "objectID": "reference/llm_sentiment.html#llm_sentiment",
    "href": "reference/llm_sentiment.html#llm_sentiment",
    "title": "Sentiment analysis",
    "section": "llm_sentiment",
    "text": "llm_sentiment"
  },
  {
    "objectID": "reference/llm_sentiment.html#description",
    "href": "reference/llm_sentiment.html#description",
    "title": "Sentiment analysis",
    "section": "Description",
    "text": "Description\nUse a Large Language Model (LLM) to perform sentiment analysis from the provided text"
  },
  {
    "objectID": "reference/llm_sentiment.html#usage",
    "href": "reference/llm_sentiment.html#usage",
    "title": "Sentiment analysis",
    "section": "Usage",
    "text": "Usage\n \nllm_sentiment( \n  .data, \n  col, \n  options = c(\"positive\", \"negative\", \"neutral\"), \n  pred_name = \".sentiment\", \n  additional_prompt = \"\" \n) \n \nllm_vec_sentiment( \n  x, \n  options = c(\"positive\", \"negative\", \"neutral\"), \n  additional_prompt = \"\", \n  preview = FALSE \n)"
  },
  {
    "objectID": "reference/llm_sentiment.html#arguments",
    "href": "reference/llm_sentiment.html#arguments",
    "title": "Sentiment analysis",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\n.data\nA data.frame or tbl object that contains the text to be analyzed\n\n\ncol\nThe name of the field to analyze, supports tidy-eval\n\n\noptions\nA vector with the options that the LLM should use to assign a sentiment to the text. Defaults to: ‘positive’, ‘negative’, ‘neutral’\n\n\npred_name\nA character vector with the name of the new column where the prediction will be placed\n\n\nadditional_prompt\nInserts this text into the prompt sent to the LLM\n\n\nx\nA vector that contains the text to be analyzed\n\n\npreview\nIt returns the R call that would have been used to run the prediction. It only returns the first record in x. Defaults to FALSE Applies to vector function only."
  },
  {
    "objectID": "reference/llm_sentiment.html#value",
    "href": "reference/llm_sentiment.html#value",
    "title": "Sentiment analysis",
    "section": "Value",
    "text": "Value\nllm_sentiment returns a data.frame or tbl object. llm_vec_sentiment returns a vector that is the same length as x."
  },
  {
    "objectID": "reference/llm_sentiment.html#examples",
    "href": "reference/llm_sentiment.html#examples",
    "title": "Sentiment analysis",
    "section": "Examples",
    "text": "Examples\n\n \nlibrary(mall) \n \ndata(\"reviews\") \n \nllm_use(\"ollama\", \"llama3.2\", seed = 100, .silent = TRUE) \n \nllm_sentiment(reviews, review) \n#&gt; # A tibble: 3 × 2\n#&gt;   review                                                              .sentiment\n#&gt;   &lt;chr&gt;                                                               &lt;chr&gt;     \n#&gt; 1 This has been the best TV I've ever used. Great screen, and sound.  positive  \n#&gt; 2 I regret buying this laptop. It is too slow and the keyboard is to… negative  \n#&gt; 3 Not sure how to feel about my new washing machine. Great color, bu… neutral\n \n# Use 'pred_name' to customize the new column's name \nllm_sentiment(reviews, review, pred_name = \"review_sentiment\") \n#&gt; # A tibble: 3 × 2\n#&gt;   review                                                        review_sentiment\n#&gt;   &lt;chr&gt;                                                         &lt;chr&gt;           \n#&gt; 1 This has been the best TV I've ever used. Great screen, and … positive        \n#&gt; 2 I regret buying this laptop. It is too slow and the keyboard… negative        \n#&gt; 3 Not sure how to feel about my new washing machine. Great col… neutral\n \n# Pass custom sentiment options \nllm_sentiment(reviews, review, c(\"positive\", \"negative\")) \n#&gt; # A tibble: 3 × 2\n#&gt;   review                                                              .sentiment\n#&gt;   &lt;chr&gt;                                                               &lt;chr&gt;     \n#&gt; 1 This has been the best TV I've ever used. Great screen, and sound.  positive  \n#&gt; 2 I regret buying this laptop. It is too slow and the keyboard is to… negative  \n#&gt; 3 Not sure how to feel about my new washing machine. Great color, bu… negative\n \n# Specify values to return per sentiment \nllm_sentiment(reviews, review, c(\"positive\" ~ 1, \"negative\" ~ 0)) \n#&gt; # A tibble: 3 × 2\n#&gt;   review                                                              .sentiment\n#&gt;   &lt;chr&gt;                                                                    &lt;dbl&gt;\n#&gt; 1 This has been the best TV I've ever used. Great screen, and sound.           1\n#&gt; 2 I regret buying this laptop. It is too slow and the keyboard is to…          0\n#&gt; 3 Not sure how to feel about my new washing machine. Great color, bu…          0\n \n# For character vectors, instead of a data frame, use this function \nllm_vec_sentiment(c(\"I am happy\", \"I am sad\")) \n#&gt; [1] \"positive\" \"negative\"\n \n# To preview the first call that will be made to the downstream R function \nllm_vec_sentiment(c(\"I am happy\", \"I am sad\"), preview = TRUE) \n#&gt; ollamar::chat(messages = list(list(role = \"user\", content = \"You are a helpful sentiment engine. Return only one of the following answers: positive, negative, neutral. No capitalization. No explanations.  The answer is based on the following text:\\nI am happy\")), \n#&gt;     output = \"text\", model = \"llama3.2\", seed = 100)"
  },
  {
    "objectID": "reference/llm_extract.html",
    "href": "reference/llm_extract.html",
    "title": "Extract entities from text",
    "section": "",
    "text": "R/llm-extract.R"
  },
  {
    "objectID": "reference/llm_extract.html#llm_extract",
    "href": "reference/llm_extract.html#llm_extract",
    "title": "Extract entities from text",
    "section": "llm_extract",
    "text": "llm_extract"
  },
  {
    "objectID": "reference/llm_extract.html#description",
    "href": "reference/llm_extract.html#description",
    "title": "Extract entities from text",
    "section": "Description",
    "text": "Description\nUse a Large Language Model (LLM) to extract specific entity, or entities, from the provided text"
  },
  {
    "objectID": "reference/llm_extract.html#usage",
    "href": "reference/llm_extract.html#usage",
    "title": "Extract entities from text",
    "section": "Usage",
    "text": "Usage\n \nllm_extract( \n  .data, \n  col, \n  labels, \n  expand_cols = FALSE, \n  additional_prompt = \"\", \n  pred_name = \".extract\" \n) \n \nllm_vec_extract(x, labels = c(), additional_prompt = \"\", preview = FALSE)"
  },
  {
    "objectID": "reference/llm_extract.html#arguments",
    "href": "reference/llm_extract.html#arguments",
    "title": "Extract entities from text",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\n.data\nA data.frame or tbl object that contains the text to be analyzed\n\n\ncol\nThe name of the field to analyze, supports tidy-eval\n\n\nlabels\nA vector with the entities to extract from the text\n\n\nexpand_cols\nIf multiple labels are passed, this is a flag that tells the function to create a new column per item in labels. If labels is a named vector, this function will use those names as the new column names, if not, the function will use a sanitized version of the content as the name.\n\n\nadditional_prompt\nInserts this text into the prompt sent to the LLM\n\n\npred_name\nA character vector with the name of the new column where the prediction will be placed\n\n\nx\nA vector that contains the text to be analyzed\n\n\npreview\nIt returns the R call that would have been used to run the prediction. It only returns the first record in x. Defaults to FALSE Applies to vector function only."
  },
  {
    "objectID": "reference/llm_extract.html#value",
    "href": "reference/llm_extract.html#value",
    "title": "Extract entities from text",
    "section": "Value",
    "text": "Value\nllm_extract returns a data.frame or tbl object. llm_vec_extract returns a vector that is the same length as x."
  },
  {
    "objectID": "reference/llm_extract.html#examples",
    "href": "reference/llm_extract.html#examples",
    "title": "Extract entities from text",
    "section": "Examples",
    "text": "Examples\n\n \nlibrary(mall) \n \ndata(\"reviews\") \n \nllm_use(\"ollama\", \"llama3.2\", seed = 100, .silent = TRUE) \n \n# Use 'labels' to let the function know what to extract \nllm_extract(reviews, review, labels = \"product\") \n#&gt; # A tibble: 3 × 2\n#&gt;   review                                        .extract       \n#&gt;   &lt;chr&gt;                                         &lt;chr&gt;          \n#&gt; 1 This has been the best TV I've ever used. Gr… tv             \n#&gt; 2 I regret buying this laptop. It is too slow … laptop         \n#&gt; 3 Not sure how to feel about my new washing ma… washing machine\n \n# Use 'pred_name' to customize the new column's name \nllm_extract(reviews, review, \"product\", pred_name = \"prod\") \n#&gt; # A tibble: 3 × 2\n#&gt;   review                                        prod           \n#&gt;   &lt;chr&gt;                                         &lt;chr&gt;          \n#&gt; 1 This has been the best TV I've ever used. Gr… tv             \n#&gt; 2 I regret buying this laptop. It is too slow … laptop         \n#&gt; 3 Not sure how to feel about my new washing ma… washing machine\n \n# Pass a vector to request multiple things, the results will be pipe delimeted \n# in a single column \nllm_extract(reviews, review, c(\"product\", \"feelings\")) \n#&gt; # A tibble: 3 × 2\n#&gt;   review                                        .extract                   \n#&gt;   &lt;chr&gt;                                         &lt;chr&gt;                      \n#&gt; 1 This has been the best TV I've ever used. Gr… tv | great                 \n#&gt; 2 I regret buying this laptop. It is too slow … laptop|frustration         \n#&gt; 3 Not sure how to feel about my new washing ma… washing machine | confusion\n \n# To get multiple columns, use 'expand_cols' \nllm_extract(reviews, review, c(\"product\", \"feelings\"), expand_cols = TRUE) \n#&gt; # A tibble: 3 × 3\n#&gt;   review                                        product            feelings     \n#&gt;   &lt;chr&gt;                                         &lt;chr&gt;              &lt;chr&gt;        \n#&gt; 1 This has been the best TV I've ever used. Gr… \"tv \"              \" great\"     \n#&gt; 2 I regret buying this laptop. It is too slow … \"laptop\"           \"frustration\"\n#&gt; 3 Not sure how to feel about my new washing ma… \"washing machine \" \" confusion\"\n \n# Pass a named vector to set the resulting column names \nllm_extract( \n  .data = reviews, \n  col = review, \n  labels = c(prod = \"product\", feels = \"feelings\"), \n  expand_cols = TRUE \n) \n#&gt; # A tibble: 3 × 3\n#&gt;   review                                        prod               feels        \n#&gt;   &lt;chr&gt;                                         &lt;chr&gt;              &lt;chr&gt;        \n#&gt; 1 This has been the best TV I've ever used. Gr… \"tv \"              \" great\"     \n#&gt; 2 I regret buying this laptop. It is too slow … \"laptop\"           \"frustration\"\n#&gt; 3 Not sure how to feel about my new washing ma… \"washing machine \" \" confusion\"\n \n# For character vectors, instead of a data frame, use this function \nllm_vec_extract(\"bob smith, 123 3rd street\", c(\"name\", \"address\")) \n#&gt; [1] \"bob smith | 123 3rd street\"\n \n# To preview the first call that will be made to the downstream R function \nllm_vec_extract( \n  \"bob smith, 123 3rd street\", \n  c(\"name\", \"address\"), \n  preview = TRUE \n) \n#&gt; ollamar::chat(messages = list(list(role = \"user\", content = \"You are a helpful text extraction engine. Extract the name, address being referred to on the text. I expect 2 items exactly. No capitalization. No explanations. Return the response exclusively in a pipe separated list, and no headers.   The answer is based on the following text:\\nbob smith, 123 3rd street\")), \n#&gt;     output = \"text\", model = \"llama3.2\", seed = 100)"
  },
  {
    "objectID": "r/LICENSE.html",
    "href": "r/LICENSE.html",
    "title": "MIT License",
    "section": "",
    "text": "MIT License\nCopyright (c) 2024 mall authors\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "articles/performance.html",
    "href": "articles/performance.html",
    "title": "Performance",
    "section": "",
    "text": "We will briefly cover this methods performance from two perspectives:\n\nHow long the analysis takes to run locally\nHow well it predicts\n\nTo do so, we will use the data_bookReviews data set, provided by the classmap package. For this exercise, only the first 100, of the total 1,000, are going to be part of this analysis.\n\nlibrary(mall)\nlibrary(classmap)\nlibrary(dplyr)\n\ndata(data_bookReviews)\n\ndata_bookReviews |&gt;\n  glimpse()\n#&gt; Rows: 1,000\n#&gt; Columns: 2\n#&gt; $ review    &lt;chr&gt; \"i got this as both a book and an audio file. i had waited t…\n#&gt; $ sentiment &lt;fct&gt; 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, …\n\nAs per the docs, sentiment is a factor indicating the sentiment of the review: negative (1) or positive (2)\n\nlength(strsplit(paste(head(data_bookReviews$review, 100), collapse = \" \"), \" \")[[1]])\n#&gt; [1] 20470\n\nJust to get an idea of how much data we’re processing, I’m using a very, very simple word count. So we’re analyzing a bit over 20 thousand words.\n\nreviews_llm &lt;- data_bookReviews |&gt;\n  head(100) |&gt; \n  llm_sentiment(\n    col = review,\n    options = c(\"positive\" ~ 2, \"negative\" ~ 1),\n    pred_name = \"predicted\"\n  )\n#&gt; ! There were 2 predictions with invalid output, they were coerced to NA\n\nAs far as time, on my Apple M3 machine, it took about 1.5 minutes to process, 100 rows, containing 20 thousand words. Setting temp to 0 in llm_use(), made the model run faster.\nThe package uses purrr to send each prompt individually to the LLM. But, I did try a few different ways to speed up the process, unsuccessfully:\n\nUsed furrr to send multiple requests at a time. This did not work because either the LLM or Ollama processed all my requests serially. So there was no improvement.\nI also tried sending more than one row’s text at a time. This cause instability in the number of results. For example sending 5 at a time, sometimes returned 7 or 8. Even sending 2 was not stable.\n\nThis is what the new table looks like:\n\nreviews_llm\n#&gt; # A tibble: 100 × 3\n#&gt;    review                                        sentiment             predicted\n#&gt;    &lt;chr&gt;                                         &lt;fct&gt;                     &lt;dbl&gt;\n#&gt;  1 \"i got this as both a book and an audio file… 1                             1\n#&gt;  2 \"this book places too much emphasis on spend… 1                             1\n#&gt;  3 \"remember the hollywood blacklist? the holly… 2                             2\n#&gt;  4 \"while i appreciate what tipler was attempti… 1                             1\n#&gt;  5 \"the others in the series were great, and i … 1                             1\n#&gt;  6 \"a few good things, but she's lost her edge … 1                             1\n#&gt;  7 \"words cannot describe how ripped off and di… 1                             1\n#&gt;  8 \"1. the persective of most writers is shaped… 1                            NA\n#&gt;  9 \"i have been a huge fan of michael crichton … 1                             1\n#&gt; 10 \"i saw dr. polk on c-span a month or two ago… 2                             2\n#&gt; # ℹ 90 more rows\n\nI used yardstick to see how well the model performed. Of course, the accuracy will not be of the “truth”, but rather the package’s results recorded in sentiment.\n\nlibrary(forcats)\n\nreviews_llm |&gt;\n  mutate(predicted = as.factor(predicted)) |&gt;\n  yardstick::accuracy(sentiment, predicted)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.980",
    "crumbs": [
      "Performance"
    ]
  },
  {
    "objectID": "articles/performance.html#performance",
    "href": "articles/performance.html#performance",
    "title": "Performance",
    "section": "",
    "text": "We will briefly cover this methods performance from two perspectives:\n\nHow long the analysis takes to run locally\nHow well it predicts\n\nTo do so, we will use the data_bookReviews data set, provided by the classmap package. For this exercise, only the first 100, of the total 1,000, are going to be part of this analysis.\n\nlibrary(mall)\nlibrary(classmap)\nlibrary(dplyr)\n\ndata(data_bookReviews)\n\ndata_bookReviews |&gt;\n  glimpse()\n#&gt; Rows: 1,000\n#&gt; Columns: 2\n#&gt; $ review    &lt;chr&gt; \"i got this as both a book and an audio file. i had waited t…\n#&gt; $ sentiment &lt;fct&gt; 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, …\n\nAs per the docs, sentiment is a factor indicating the sentiment of the review: negative (1) or positive (2)\n\nlength(strsplit(paste(head(data_bookReviews$review, 100), collapse = \" \"), \" \")[[1]])\n#&gt; [1] 20470\n\nJust to get an idea of how much data we’re processing, I’m using a very, very simple word count. So we’re analyzing a bit over 20 thousand words.\n\nreviews_llm &lt;- data_bookReviews |&gt;\n  head(100) |&gt; \n  llm_sentiment(\n    col = review,\n    options = c(\"positive\" ~ 2, \"negative\" ~ 1),\n    pred_name = \"predicted\"\n  )\n#&gt; ! There were 2 predictions with invalid output, they were coerced to NA\n\nAs far as time, on my Apple M3 machine, it took about 1.5 minutes to process, 100 rows, containing 20 thousand words. Setting temp to 0 in llm_use(), made the model run faster.\nThe package uses purrr to send each prompt individually to the LLM. But, I did try a few different ways to speed up the process, unsuccessfully:\n\nUsed furrr to send multiple requests at a time. This did not work because either the LLM or Ollama processed all my requests serially. So there was no improvement.\nI also tried sending more than one row’s text at a time. This cause instability in the number of results. For example sending 5 at a time, sometimes returned 7 or 8. Even sending 2 was not stable.\n\nThis is what the new table looks like:\n\nreviews_llm\n#&gt; # A tibble: 100 × 3\n#&gt;    review                                        sentiment             predicted\n#&gt;    &lt;chr&gt;                                         &lt;fct&gt;                     &lt;dbl&gt;\n#&gt;  1 \"i got this as both a book and an audio file… 1                             1\n#&gt;  2 \"this book places too much emphasis on spend… 1                             1\n#&gt;  3 \"remember the hollywood blacklist? the holly… 2                             2\n#&gt;  4 \"while i appreciate what tipler was attempti… 1                             1\n#&gt;  5 \"the others in the series were great, and i … 1                             1\n#&gt;  6 \"a few good things, but she's lost her edge … 1                             1\n#&gt;  7 \"words cannot describe how ripped off and di… 1                             1\n#&gt;  8 \"1. the persective of most writers is shaped… 1                            NA\n#&gt;  9 \"i have been a huge fan of michael crichton … 1                             1\n#&gt; 10 \"i saw dr. polk on c-span a month or two ago… 2                             2\n#&gt; # ℹ 90 more rows\n\nI used yardstick to see how well the model performed. Of course, the accuracy will not be of the “truth”, but rather the package’s results recorded in sentiment.\n\nlibrary(forcats)\n\nreviews_llm |&gt;\n  mutate(predicted = as.factor(predicted)) |&gt;\n  yardstick::accuracy(sentiment, predicted)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.980",
    "crumbs": [
      "Performance"
    ]
  },
  {
    "objectID": "articles/caching.html",
    "href": "articles/caching.html",
    "title": "Caching results",
    "section": "",
    "text": "Data preparation, and model preparation, is usually a iterative process. Because models in R are normally rather fast, it is not a problem to re-run the entire code to confirm that all of the results are reproducible. But in the case of LLM’s, re-running things may be a problem. Locally, running the LLM will be processor intensive, and typically long. If running against a remote LLM, the issue would the cost per token.\nTo ameliorate this, mall is able to cache existing results in a folder. That way, running the same analysis over and over, will be much quicker. Because instead of calling the LLM again, mall will return the previously recorded result.\nBy default, this functionality is turned on. The results will be saved to a folder named “_mall_cache” . The name of the folder can be easily changed, simply set the .cache argument in llm_use(). To disable this functionality, set the argument to an empty character, meaning .cache = \"\".",
    "crumbs": [
      "Caching"
    ]
  },
  {
    "objectID": "articles/caching.html#how-it-works",
    "href": "articles/caching.html#how-it-works",
    "title": "Caching results",
    "section": "How it works",
    "text": "How it works\nmall uses all of the values used to make the LLM query as the “finger print” to confidently identify when the same query is being done again. This includes:\n\nThe value in the particular row\nThe additional prompting built by the llm_ function,\nAny other arguments/options used, set in llm_use()\nThe name of the back end used for the call\n\nA file is created that contains the request and response. The key to the process is the name of the file itself. The name is the hashed value of the combined value of the items listed above. This becomes the “finger print” that allows mall to know if there is an existing cache.",
    "crumbs": [
      "Caching"
    ]
  },
  {
    "objectID": "articles/caching.html#walk-through",
    "href": "articles/caching.html#walk-through",
    "title": "Caching results",
    "section": "Walk-through",
    "text": "Walk-through\nWe will initialize the LLM session specifying a seed\n\nlibrary(mall)\n\nllm_use(\"ollama\", \"llama3.1\", seed = 100)\n#&gt; \n#&gt; ── mall session object\n#&gt; Backend: ollama\n#&gt; LLM session:\n#&gt;   model:llama3.1\n#&gt; \n#&gt;   seed:100\n#&gt; \n#&gt; R session: cache_folder:_mall_cache\n\nUsing the tictoc package, we will measure how long it takes to make a simple sentiment call.\n\nlibrary(tictoc)\n\ntic()\nllm_vec_sentiment(\"I am happy\")\n#&gt; [1] \"positive\"\ntoc()\n#&gt; 1.266 sec elapsed\n\nThis creates a the “_mall_cache” folder, and inside a sub-folder, it creates a file with the cache. The name of the file is the resulting hash value of the combination mentioned in the previous section.\n\ndir_ls(\"_mall_cache\", recurse = TRUE, type = \"file\")\n#&gt; _mall_cache/08/086214f2638f60496fd0468d7de37c59.json\n\nThe cache is a JSON file, that contains both the request, and the response. As mentioned in the previous section, the named of the file is derived from the combining the values in the request ($request).\n\njsonlite::read_json(\n  \"_mall_cache/08/086214f2638f60496fd0468d7de37c59.json\", \n  simplifyVector = TRUE, \n  flatten = TRUE\n  )\n#&gt; $request\n#&gt; $request$messages\n#&gt;   role\n#&gt; 1 user\n#&gt;                                                                                                                                                                                                  content\n#&gt; 1 You are a helpful sentiment engine. Return only one of the following answers: positive, negative, neutral. No capitalization. No explanations.  The answer is based on the following text:\\nI am happy\n#&gt; \n#&gt; $request$output\n#&gt; [1] \"text\"\n#&gt; \n#&gt; $request$model\n#&gt; [1] \"llama3.1\"\n#&gt; \n#&gt; $request$seed\n#&gt; [1] 100\n#&gt; \n#&gt; \n#&gt; $response\n#&gt; [1] \"positive\"\n\nRe-running the same mall call, will complete significantly faster\n\ntic()\nllm_vec_sentiment(\"I am happy\")\n#&gt; [1] \"positive\"\ntoc()\n#&gt; 0.001 sec elapsed\n\nIf a slightly different query is made, mall will recognize that this is a different call, and it will send it to the LLM. The results are then saved in a new JSON file.\n\nllm_vec_sentiment(\"I am very happy\")\n#&gt; [1] \"positive\"\n\ndir_ls(\"_mall_cache\", recurse = TRUE, type = \"file\")\n#&gt; _mall_cache/08/086214f2638f60496fd0468d7de37c59.json\n#&gt; _mall_cache/7c/7c7cfcfddc43a90b4deb9d7e60e88291.json\n\nDuring the same R session, if we change something in llm_use() that will impact the request to the LLM, that will trigger a new cache file\n\nllm_use(seed = 101)\n#&gt; \n#&gt; ── mall session object\n#&gt; Backend: ollama\n#&gt; LLM session:\n#&gt;   model:llama3.1\n#&gt; \n#&gt;   seed:101\n#&gt; \n#&gt; R session: cache_folder:_mall_cache\n\nllm_vec_sentiment(\"I am very happy\")\n#&gt; [1] \"positive\"\n\ndir_ls(\"_mall_cache\", recurse = TRUE, type = \"file\")\n#&gt; _mall_cache/08/086214f2638f60496fd0468d7de37c59.json\n#&gt; _mall_cache/7c/7c7cfcfddc43a90b4deb9d7e60e88291.json\n#&gt; _mall_cache/f1/f1c72c2bf22e22074cef9c859d6344a6.json\n\nThe only argument that does not trigger a new cache file is .silent\n\nllm_use(seed = 101, .silent = TRUE)\n\nllm_vec_sentiment(\"I am very happy\")\n#&gt; [1] \"positive\"\n\ndir_ls(\"_mall_cache\", recurse = TRUE, type = \"file\")\n#&gt; _mall_cache/08/086214f2638f60496fd0468d7de37c59.json\n#&gt; _mall_cache/7c/7c7cfcfddc43a90b4deb9d7e60e88291.json\n#&gt; _mall_cache/f1/f1c72c2bf22e22074cef9c859d6344a6.json",
    "crumbs": [
      "Caching"
    ]
  },
  {
    "objectID": "articles/caching.html#performance-improvements",
    "href": "articles/caching.html#performance-improvements",
    "title": "Caching results",
    "section": "Performance improvements",
    "text": "Performance improvements\nTo drive home the point of the usefulness of this feature, we will use the same data set we used for the README. To start, we will change the cache folder to make it easy to track the new files\n\nllm_use(.cache = \"_performance_cache\", .silent = TRUE)\n\nAs mentioned, we will use the data_bookReviews data frame from the classmap package\n\nlibrary(classmap)\n\ndata(data_bookReviews)\n\nThe individual reviews in this data set are really long. So they take a while to process. To run this test, we will use the first 5 rows:\n\ntic()\n\ndata_bookReviews |&gt;\n  head(5)  |&gt; \n  llm_sentiment(review)\n#&gt; # A tibble: 5 × 3\n#&gt;   review                                        sentiment .sentiment\n#&gt;   &lt;chr&gt;                                         &lt;fct&gt;     &lt;chr&gt;     \n#&gt; 1 \"i got this as both a book and an audio file… 1         negative  \n#&gt; 2 \"this book places too much emphasis on spend… 1         negative  \n#&gt; 3 \"remember the hollywood blacklist? the holly… 2         negative  \n#&gt; 4 \"while i appreciate what tipler was attempti… 1         negative  \n#&gt; 5 \"the others in the series were great, and i … 1         negative\n\ntoc()\n#&gt; 10.223 sec elapsed\n\nThe analysis took about 10 seconds on my laptop, so around 2 seconds per record. That may not seem like much, but during model, or workflow, development having to wait this long every time will take its toll on our time, and patience.\nThe new cache folder now has the 5 records cached in their corresponding JSON files\n\ndir_ls(\"_performance_cache\", recurse = TRUE, type = \"file\")\n#&gt; _performance_cache/23/23ea4fff55a6058db3b4feefe447ddeb.json\n#&gt; _performance_cache/60/60a0dbb7d3b8133d40e2f74deccdbf47.json\n#&gt; _performance_cache/76/76f1b84b70328b1b3533436403914217.json\n#&gt; _performance_cache/c7/c7cf6e0f9683ae29eba72b0a4dd4b189.json\n#&gt; _performance_cache/e3/e375559b424833d17c7bcb067fe6b0f8.json\n\nRe-running the same exact call will not take a fraction of a fraction of the original time!\n\ntic()\n\ndata_bookReviews |&gt;\n  head(5)  |&gt; \n  llm_sentiment(review)\n#&gt; # A tibble: 5 × 3\n#&gt;   review                                        sentiment .sentiment\n#&gt;   &lt;chr&gt;                                         &lt;fct&gt;     &lt;chr&gt;     \n#&gt; 1 \"i got this as both a book and an audio file… 1         negative  \n#&gt; 2 \"this book places too much emphasis on spend… 1         negative  \n#&gt; 3 \"remember the hollywood blacklist? the holly… 2         negative  \n#&gt; 4 \"while i appreciate what tipler was attempti… 1         negative  \n#&gt; 5 \"the others in the series were great, and i … 1         negative\n\ntoc()\n#&gt; 0.01 sec elapsed\n\nRunning an additional record, will only cost the time it takes to process it. The other 5 will still be scored using their cached result\n\ntic()\n\ndata_bookReviews |&gt;\n  head(6)  |&gt; \n  llm_sentiment(review)\n#&gt; # A tibble: 6 × 3\n#&gt;   review                                        sentiment .sentiment\n#&gt;   &lt;chr&gt;                                         &lt;fct&gt;     &lt;chr&gt;     \n#&gt; 1 \"i got this as both a book and an audio file… 1         negative  \n#&gt; 2 \"this book places too much emphasis on spend… 1         negative  \n#&gt; 3 \"remember the hollywood blacklist? the holly… 2         negative  \n#&gt; 4 \"while i appreciate what tipler was attempti… 1         negative  \n#&gt; 5 \"the others in the series were great, and i … 1         negative  \n#&gt; 6 \"a few good things, but she's lost her edge … 1         negative\n\ntoc()\n#&gt; 0.624 sec elapsed",
    "crumbs": [
      "Caching"
    ]
  },
  {
    "objectID": "articles/caching.html#set-the-seed",
    "href": "articles/caching.html#set-the-seed",
    "title": "Caching results",
    "section": "Set the seed!",
    "text": "Set the seed!\nIf at the end of your analysis, you plan to re-run all of the code, and you want to take advantage of the caching functionaly, then set the model seed. This will allow for the exact same results to be returned by the LLM.\nIf no seed is set during development, then the results will always come back the same because the cache is being read. But once the cache is removed, to run everything from 0, then you will get different results. This is because the invariability of the cache results, mask the fact that the model will have variability.\n\nllm_use(\"ollama\", \"llama3.1\", seed = 999)\n#&gt; \n#&gt; ── mall session object\n#&gt; Backend: ollama\n#&gt; LLM session:\n#&gt;   model:llama3.1\n#&gt; \n#&gt;   seed:999\n#&gt; \n#&gt; R session: cache_folder:_performance_cache",
    "crumbs": [
      "Caching"
    ]
  },
  {
    "objectID": "articles/databricks.html",
    "href": "articles/databricks.html",
    "title": "Databricks",
    "section": "",
    "text": "This brief example shows how seamless it is to use the same functions, but against a remote database connection. Today, it works with the following functions:",
    "crumbs": [
      "Databricks (R only)"
    ]
  },
  {
    "objectID": "articles/databricks.html#examples",
    "href": "articles/databricks.html#examples",
    "title": "Databricks",
    "section": "Examples",
    "text": "Examples\nWe will start by connecting to the Databricks Warehouse\n\nlibrary(mall)\nlibrary(DBI)\n\ncon &lt;- dbConnect(\n  odbc::databricks(),\n  HTTPPath = Sys.getenv(\"DATABRICKS_PATH\")\n)\n\nNext, we will create a small reviews table\n\nlibrary(dplyr)\n\nreviews &lt;- tribble(\n  ~review,\n  \"This has been the best TV I've ever used. Great screen, and sound.\",\n  \"I regret buying this laptop. It is too slow and the keyboard is too noisy\",\n  \"Not sure how to feel about my new washing machine. Great color, but hard to figure\"\n)\n\ntbl_reviews &lt;- copy_to(con, reviews, overwrite = TRUE)\n\nUsing llm_sentiment() in Databricks will call that vendor’s SQL AI function directly:\n\ntbl_reviews |&gt;\n  llm_sentiment(review)\n#&gt; # Source:   SQL [3 x 2]\n#&gt; # Database: Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#&gt;   review                                                              .sentiment\n#&gt;   &lt;chr&gt;                                                               &lt;chr&gt;     \n#&gt; 1 This has been the best TV Ive ever used. Great screen, and sound.   positive  \n#&gt; 2 I regret buying this laptop. It is too slow and the keyboard is to… negative  \n#&gt; 3 Not sure how to feel about my new washing machine. Great color, bu… mixed\n\nThere are some differences in the arguments, and output of the LLM’s. Notice that instead of “neutral”, the prediction is “mixed”. The AI Sentiment function does not allow to change the possible options.\nNext, we will try llm_summarize(). The max_words argument maps to the same argument in the AI Summarize function:\n\ntbl_reviews |&gt;\n  llm_summarize(review, max_words = 5) |&gt; \n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT `reviews`.*, ai_summarize(`review`, CAST(5.0 AS INT)) AS `.summary`\n#&gt; FROM `reviews`\n\nllm_classify() for this back-end, will only accept unnamed options.\n\ntbl_reviews |&gt; \n  llm_classify(review, c(\"appliance\", \"computer\"))\n#&gt; # Source:   SQL [3 x 2]\n#&gt; # Database: Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#&gt;   review                                                               .classify\n#&gt;   &lt;chr&gt;                                                                &lt;chr&gt;    \n#&gt; 1 This has been the best TV Ive ever used. Great screen, and sound.    appliance\n#&gt; 2 I regret buying this laptop. It is too slow and the keyboard is too… computer \n#&gt; 3 Not sure how to feel about my new washing machine. Great color, but… appliance",
    "crumbs": [
      "Databricks (R only)"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "mall",
    "section": "",
    "text": "Run multiple LLM predictions against a data frame. The predictions are processed row-wise over a specified column. It works using a pre-determined one-shot prompt, along with the current row’s content. mall has been implemented for both R and Python. The prompt that is use will depend of the type of analysis needed.\nCurrently, the included prompts perform the following:\nThis package is inspired by the SQL AI functions now offered by vendors such as Databricks and Snowflake. mall uses Ollama to interact with LLMs installed locally.\nFor R, that interaction takes place via the ollamar package. The functions are designed to easily work with piped commands, such as dplyr.\nFor Python, mall is a library extension to Polars. To interact with Ollama, it uses the official Python library."
  },
  {
    "objectID": "index.html#motivation",
    "href": "index.html#motivation",
    "title": "mall",
    "section": "Motivation",
    "text": "Motivation\nWe want to new find ways to help data scientists use LLMs in their daily work. Unlike the familiar interfaces, such as chatting and code completion, this interface runs your text data directly against the LLM.\nThe LLM’s flexibility, allows for it to adapt to the subject of your data, and provide surprisingly accurate predictions. This saves the data scientist the need to write and tune an NLP model.\nIn recent times, the capabilities of LLMs that can run locally in your computer have increased dramatically. This means that these sort of analysis can run in your machine with good accuracy. Additionally, it makes it possible to take advantage of LLM’s at your institution, since the data will not leave the corporate network."
  },
  {
    "objectID": "index.html#get-started",
    "href": "index.html#get-started",
    "title": "mall",
    "section": "Get started",
    "text": "Get started\n\nInstall mall from Github\n\n\nRPython\n\n\npak::pak(\"mlverse/mall/r\")\n\n\npip install \"mall @ git+https://git@github.com/mlverse/mall.git#subdirectory=python\"\n\n\n\n\nDownload Ollama from the official website\nInstall and start Ollama in your computer\n\n\nRPython\n\n\n\nInstall Ollama in your machine. The ollamar package’s website provides this Installation guide\nDownload an LLM model. For example, I have been developing this package using Llama 3.2 to test. To get that model you can run:\nollamar::pull(\"llama3.2\")\n\n\n\n\nInstall the official Ollama library\npip install ollama\nDownload an LLM model. For example, I have been developing this package using Llama 3.2 to test. To get that model you can run:\nimport ollama\nollama.pull('llama3.2')\n\n\n\n\n\nWith Databricks (R only)\nIf you pass a table connected to Databricks via odbc, mall will automatically use Databricks’ LLM instead of Ollama. You won’t need Ollama installed if you are using Databricks only.\nmall will call the appropriate SQL AI function. For more information see our Databricks article."
  },
  {
    "objectID": "index.html#llm-functions",
    "href": "index.html#llm-functions",
    "title": "mall",
    "section": "LLM functions",
    "text": "LLM functions\nWe will start with loading a very small data set contained in mall. It has 3 product reviews that we will use as the source of our examples.\n\nRPython\n\n\n\nlibrary(mall)\ndata(\"reviews\")\n\nreviews\n#&gt; # A tibble: 3 × 1\n#&gt;   review                                                                        \n#&gt;   &lt;chr&gt;                                                                         \n#&gt; 1 This has been the best TV I've ever used. Great screen, and sound.            \n#&gt; 2 I regret buying this laptop. It is too slow and the keyboard is too noisy     \n#&gt; 3 Not sure how to feel about my new washing machine. Great color, but hard to f…\n\n\n\n\nimport mall \ndata = mall.MallData\nreviews = data.reviews\n\nreviews \n\n\n\n\n\n\nreview\n\n\n\n\n\"This has been the best TV I've ever used. Great screen, and sound.\"\n\n\n\"I regret buying this laptop. It is too slow and the keyboard is too noisy\"\n\n\n\"Not sure how to feel about my new washing machine. Great color, but hard to figure\"\n\n\n\n\n\n\n\n\n\n\nSentiment\nAutomatically returns “positive”, “negative”, or “neutral” based on the text.\n\nRPython\n\n\n\nreviews |&gt;\n  llm_sentiment(review)\n#&gt; # A tibble: 3 × 2\n#&gt;   review                                                              .sentiment\n#&gt;   &lt;chr&gt;                                                               &lt;chr&gt;     \n#&gt; 1 This has been the best TV I've ever used. Great screen, and sound.  positive  \n#&gt; 2 I regret buying this laptop. It is too slow and the keyboard is to… negative  \n#&gt; 3 Not sure how to feel about my new washing machine. Great color, bu… neutral\n\nFor more information and examples visit this function’s R reference page\n\n\n\nreviews.llm.sentiment(\"review\")\n\n\n\n\n\n\nreview\nsentiment\n\n\n\n\n\"This has been the best TV I've ever used. Great screen, and sound.\"\n\"positive\"\n\n\n\"I regret buying this laptop. It is too slow and the keyboard is too noisy\"\n\"negative\"\n\n\n\"Not sure how to feel about my new washing machine. Great color, but hard to figure\"\n\"neutral\"\n\n\n\n\n\n\nFor more information and examples visit this function’s Python reference page\n\n\n\n\n\nSummarize\nThere may be a need to reduce the number of words in a given text. Typically to make it easier to understand its intent. The function has an argument to control the maximum number of words to output (max_words):\n\nRPython\n\n\n\nreviews |&gt;\n  llm_summarize(review, max_words = 5)\n#&gt; # A tibble: 3 × 2\n#&gt;   review                                        .summary                      \n#&gt;   &lt;chr&gt;                                         &lt;chr&gt;                         \n#&gt; 1 This has been the best TV I've ever used. Gr… it's a great tv               \n#&gt; 2 I regret buying this laptop. It is too slow … laptop purchase was a mistake \n#&gt; 3 Not sure how to feel about my new washing ma… having mixed feelings about it\n\nFor more information and examples visit this function’s R reference page\n\n\n\nreviews.llm.summarize(\"review\", 5)\n\n\n\n\n\n\nreview\nsummary\n\n\n\n\n\"This has been the best TV I've ever used. Great screen, and sound.\"\n\"great tv with good features\"\n\n\n\"I regret buying this laptop. It is too slow and the keyboard is too noisy\"\n\"laptop purchase was a mistake\"\n\n\n\"Not sure how to feel about my new washing machine. Great color, but hard to figure\"\n\"feeling uncertain about new purchase\"\n\n\n\n\n\n\nFor more information and examples visit this function’s Python reference page\n\n\n\n\n\nClassify\nUse the LLM to categorize the text into one of the options you provide:\n\nRPython\n\n\n\nreviews |&gt;\n  llm_classify(review, c(\"appliance\", \"computer\"))\n#&gt; # A tibble: 3 × 2\n#&gt;   review                                        .classify\n#&gt;   &lt;chr&gt;                                         &lt;chr&gt;    \n#&gt; 1 This has been the best TV I've ever used. Gr… computer \n#&gt; 2 I regret buying this laptop. It is too slow … computer \n#&gt; 3 Not sure how to feel about my new washing ma… appliance\n\nFor more information and examples visit this function’s R reference page\n\n\n\nreviews.llm.classify(\"review\", [\"computer\", \"appliance\"])\n\n\n\n\n\n\nreview\nclassify\n\n\n\n\n\"This has been the best TV I've ever used. Great screen, and sound.\"\n\"appliance\"\n\n\n\"I regret buying this laptop. It is too slow and the keyboard is too noisy\"\n\"computer\"\n\n\n\"Not sure how to feel about my new washing machine. Great color, but hard to figure\"\n\"appliance\"\n\n\n\n\n\n\nFor more information and examples visit this function’s Python reference page\n\n\n\n\n\nExtract\nOne of the most interesting use cases Using natural language, we can tell the LLM to return a specific part of the text. In the following example, we request that the LLM return the product being referred to. We do this by simply saying “product”. The LLM understands what we mean by that word, and looks for that in the text.\n\nRPython\n\n\n\nreviews |&gt;\n  llm_extract(review, \"product\")\n#&gt; # A tibble: 3 × 2\n#&gt;   review                                        .extract       \n#&gt;   &lt;chr&gt;                                         &lt;chr&gt;          \n#&gt; 1 This has been the best TV I've ever used. Gr… tv             \n#&gt; 2 I regret buying this laptop. It is too slow … laptop         \n#&gt; 3 Not sure how to feel about my new washing ma… washing machine\n\nFor more information and examples visit this function’s R reference page\n\n\n\nreviews.llm.extract(\"review\", \"product\")\n\n\n\n\n\n\nreview\nextract\n\n\n\n\n\"This has been the best TV I've ever used. Great screen, and sound.\"\n\"tv\"\n\n\n\"I regret buying this laptop. It is too slow and the keyboard is too noisy\"\n\"laptop\"\n\n\n\"Not sure how to feel about my new washing machine. Great color, but hard to figure\"\n\"washing machine\"\n\n\n\n\n\n\nFor more information and examples visit this function’s Python reference page\n\n\n\n\n\nClassify\nUse the LLM to categorize the text into one of the options you provide:\n\nRPython\n\n\n\nreviews |&gt;\n  llm_classify(review, c(\"appliance\", \"computer\"))\n#&gt; # A tibble: 3 × 2\n#&gt;   review                                        .classify\n#&gt;   &lt;chr&gt;                                         &lt;chr&gt;    \n#&gt; 1 This has been the best TV I've ever used. Gr… computer \n#&gt; 2 I regret buying this laptop. It is too slow … computer \n#&gt; 3 Not sure how to feel about my new washing ma… appliance\n\nFor more information and examples visit this function’s R reference page\n\n\n\nreviews.llm.classify(\"review\", [\"computer\", \"appliance\"])\n\n\n\n\n\n\nreview\nclassify\n\n\n\n\n\"This has been the best TV I've ever used. Great screen, and sound.\"\n\"appliance\"\n\n\n\"I regret buying this laptop. It is too slow and the keyboard is too noisy\"\n\"computer\"\n\n\n\"Not sure how to feel about my new washing machine. Great color, but hard to figure\"\n\"appliance\"\n\n\n\n\n\n\nFor more information and examples visit this function’s Python reference page\n\n\n\n\n\nVerify\nThis functions allows you to check and see if a statement is true, based on the provided text. By default, it will return a 1 for “yes”, and 0 for “no”. This can be customized.\n\nRPython\n\n\n\nreviews |&gt;\n  llm_verify(review, \"is the customer happy with the purchase\")\n#&gt; # A tibble: 3 × 2\n#&gt;   review                                                                 .verify\n#&gt;   &lt;chr&gt;                                                                  &lt;fct&gt;  \n#&gt; 1 This has been the best TV I've ever used. Great screen, and sound.     1      \n#&gt; 2 I regret buying this laptop. It is too slow and the keyboard is too n… 0      \n#&gt; 3 Not sure how to feel about my new washing machine. Great color, but h… 0\n\nFor more information and examples visit this function’s R reference page\n\n\n\nreviews.llm.verify(\"review\", \"is the customer happy with the purchase\")\n\n\n\n\n\n\nreview\nverify\n\n\n\n\n\"This has been the best TV I've ever used. Great screen, and sound.\"\n1\n\n\n\"I regret buying this laptop. It is too slow and the keyboard is too noisy\"\n0\n\n\n\"Not sure how to feel about my new washing machine. Great color, but hard to figure\"\n0\n\n\n\n\n\n\nFor more information and examples visit this function’s Python reference page\n\n\n\n\n\nTranslate\nAs the title implies, this function will translate the text into a specified language. What is really nice, it is that you don’t need to specify the language of the source text. Only the target language needs to be defined. The translation accuracy will depend on the LLM\n\nRPython\n\n\n\nreviews |&gt;\n  llm_translate(review, \"spanish\")\n#&gt; # A tibble: 3 × 2\n#&gt;   review                                        .translation                    \n#&gt;   &lt;chr&gt;                                         &lt;chr&gt;                           \n#&gt; 1 This has been the best TV I've ever used. Gr… Esta ha sido la mejor televisió…\n#&gt; 2 I regret buying this laptop. It is too slow … Me arrepiento de comprar este p…\n#&gt; 3 Not sure how to feel about my new washing ma… No estoy seguro de cómo me sien…\n\nFor more information and examples visit this function’s R reference page\n\n\n\nreviews.llm.translate(\"review\", \"spanish\")\n\n\n\n\n\n\nreview\ntranslation\n\n\n\n\n\"This has been the best TV I've ever used. Great screen, and sound.\"\n\"Esta ha sido la mejor televisión que he utilizado hasta ahora. Gran pantalla y sonido.\"\n\n\n\"I regret buying this laptop. It is too slow and the keyboard is too noisy\"\n\"Me arrepiento de comprar este portátil. Es demasiado lento y la tecla es demasiado ruidosa.\"\n\n\n\"Not sure how to feel about my new washing machine. Great color, but hard to figure\"\n\"No estoy seguro de cómo sentirme con mi nueva lavadora. Un color maravilloso, pero muy difícil de en…\n\n\n\n\n\n\nFor more information and examples visit this function’s Python reference page\n\n\n\n\n\nCustom prompt\nIt is possible to pass your own prompt to the LLM, and have mall run it against each text entry:\n\nRPython\n\n\n\nmy_prompt &lt;- paste(\n  \"Answer a question.\",\n  \"Return only the answer, no explanation\",\n  \"Acceptable answers are 'yes', 'no'\",\n  \"Answer this about the following text, is this a happy customer?:\"\n)\n\nreviews |&gt;\n  llm_custom(review, my_prompt)\n#&gt; # A tibble: 3 × 2\n#&gt;   review                                                                   .pred\n#&gt;   &lt;chr&gt;                                                                    &lt;chr&gt;\n#&gt; 1 This has been the best TV I've ever used. Great screen, and sound.       Yes  \n#&gt; 2 I regret buying this laptop. It is too slow and the keyboard is too noi… No   \n#&gt; 3 Not sure how to feel about my new washing machine. Great color, but har… No\n\nFor more information and examples visit this function’s R reference page\n\n\n\nmy_prompt = (\n    \"Answer a question.\"\n    \"Return only the answer, no explanation\"\n    \"Acceptable answers are 'yes', 'no'\"\n    \"Answer this about the following text, is this a happy customer?:\"\n)\n\nreviews.llm.custom(\"review\", prompt = my_prompt)\n\n\n\n\n\n\nreview\ncustom\n\n\n\n\n\"This has been the best TV I've ever used. Great screen, and sound.\"\n\"Yes\"\n\n\n\"I regret buying this laptop. It is too slow and the keyboard is too noisy\"\n\"No\"\n\n\n\"Not sure how to feel about my new washing machine. Great color, but hard to figure\"\n\"No\"\n\n\n\n\n\n\nFor more information and examples visit this function’s Python reference page"
  },
  {
    "objectID": "index.html#model-selection-and-settings",
    "href": "index.html#model-selection-and-settings",
    "title": "mall",
    "section": "Model selection and settings",
    "text": "Model selection and settings\nYou can set the model and its options to use when calling the LLM. In this case, we refer to options as model specific things that can be set, such as seed or temperature.\n\nRPython\n\n\nInvoking an llm function will automatically initialize a model selection if you don’t have one selected yet. If there is only one option, it will pre-select it for you. If there are more than one available models, then mall will present you as menu selection so you can select which model you wish to use.\nCalling llm_use() directly will let you specify the model and backend to use. You can also setup additional arguments that will be passed down to the function that actually runs the prediction. In the case of Ollama, that function is chat().\nThe model to use, and other options can be set for the current R session\n\nllm_use(\"ollama\", \"llama3.2\", seed = 100, temperature = 0)\n\n\n\nThe model and options to be used will be defined at the Polars data frame object level. If not passed, the default model will be llama3.2.\n\nreviews.llm.use(\"ollama\", \"llama3.2\", options = dict(seed = 100))\n\n\n\n\n\nResults caching\nBy default mall caches the requests and corresponding results from a given LLM run. Each response is saved as individual JSON files. By default, the folder name is _mall_cache. The folder name can be customized, if needed. Also, the caching can be turned off by setting the argument to empty (\"\").\n\nRPython\n\n\n\nllm_use(.cache = \"_my_cache\")\n\nTo turn off:\n\nllm_use(.cache = \"\")\n\n\n\n\nreviews.llm.use(_cache = \"my_cache\")\n\nTo turn off:\n\nreviews.llm.use(_cache = \"\")\n\n\n\n\nFor more information see the Caching Results article."
  },
  {
    "objectID": "index.html#key-considerations",
    "href": "index.html#key-considerations",
    "title": "mall",
    "section": "Key considerations",
    "text": "Key considerations\nThe main consideration is cost. Either, time cost, or money cost.\nIf using this method with an LLM locally available, the cost will be a long running time. Unless using a very specialized LLM, a given LLM is a general model. It was fitted using a vast amount of data. So determining a response for each row, takes longer than if using a manually created NLP model. The default model used in Ollama is Llama 3.2, which was fitted using 3B parameters.\nIf using an external LLM service, the consideration will need to be for the billing costs of using such service. Keep in mind that you will be sending a lot of data to be evaluated.\nAnother consideration is the novelty of this approach. Early tests are providing encouraging results. But you, as an user, will still need to keep in mind that the predictions will not be infallible, so always check the output. At this time, I think the best use for this method, is for a quick analysis."
  },
  {
    "objectID": "index.html#vector-functions-r-only",
    "href": "index.html#vector-functions-r-only",
    "title": "mall",
    "section": "Vector functions (R only)",
    "text": "Vector functions (R only)\nmall includes functions that expect a vector, instead of a table, to run the predictions. This should make it easier to test things, such as custom prompts or results of specific text. Each llm_ function has a corresponding llm_vec_ function:\n\nllm_vec_sentiment(\"I am happy\")\n#&gt; [1] \"positive\"\n\n\nllm_vec_translate(\"Este es el mejor dia!\", \"english\")\n#&gt; [1] \"It's the best day!\""
  },
  {
    "objectID": "reference/MallFrame.html",
    "href": "reference/MallFrame.html",
    "title": "MallFrame",
    "section": "",
    "text": "MallFrame(self, df)\nExtension to Polars that add ability to use an LLM to run batch predictions over a data frame\nWe will start by loading the needed libraries, and set up the data frame that will be used in the examples:\nimport mall\nimport polars as pl\npl.Config(fmt_str_lengths=100)\npl.Config.set_tbl_hide_dataframe_shape(True)\npl.Config.set_tbl_hide_column_data_types(True)\ndata = mall.MallData\nreviews = data.reviews\nreviews.llm.use(options = dict(seed = 100))"
  },
  {
    "objectID": "reference/MallFrame.html#methods",
    "href": "reference/MallFrame.html#methods",
    "title": "MallFrame",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nclassify\nClassify text into specific categories.\n\n\ncustom\nProvide the full prompt that the LLM will process.\n\n\nextract\nPull a specific label from the text.\n\n\nsentiment\nUse an LLM to run a sentiment analysis\n\n\nsummarize\nSummarize the text down to a specific number of words.\n\n\ntranslate\nTranslate text into another language.\n\n\nuse\nDefine the model, backend, and other options to use to\n\n\nverify\nCheck to see if something is true about the text.\n\n\n\n\nclassify\nMallFrame.classify(col, labels='', additional='', pred_name='classify')\nClassify text into specific categories.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncol\nstr\nThe name of the text field to process\nrequired\n\n\nlabels\nlist\nA list or a DICT object that defines the categories to classify the text as. It will return one of the provided labels.\n''\n\n\npred_name\nstr\nA character vector with the name of the new column where the prediction will be placed\n'classify'\n\n\nadditional\nstr\nInserts this text into the prompt sent to the LLM\n''\n\n\n\n\n\nExamples\n\nreviews.llm.classify(\"review\", [\"appliance\", \"computer\"])\n\n\n\n\n\n\nreview\nclassify\n\n\n\n\n\"This has been the best TV I've ever used. Great screen, and sound.\"\n\"computer\"\n\n\n\"I regret buying this laptop. It is too slow and the keyboard is too noisy\"\n\"computer\"\n\n\n\"Not sure how to feel about my new washing machine. Great color, but hard to figure\"\n\"appliance\"\n\n\n\n\n\n\n\n# Use 'pred_name' to customize the new column's name\nreviews.llm.classify(\"review\", [\"appliance\", \"computer\"], pred_name=\"prod_type\")\n\n\n\n\n\n\nreview\nprod_type\n\n\n\n\n\"This has been the best TV I've ever used. Great screen, and sound.\"\n\"computer\"\n\n\n\"I regret buying this laptop. It is too slow and the keyboard is too noisy\"\n\"computer\"\n\n\n\"Not sure how to feel about my new washing machine. Great color, but hard to figure\"\n\"appliance\"\n\n\n\n\n\n\n\n#Pass a DICT to set custom values for each classification\nreviews.llm.classify(\"review\", {\"appliance\" : \"1\", \"computer\" : \"2\"})\n\n\n\n\n\n\nreview\nclassify\n\n\n\n\n\"This has been the best TV I've ever used. Great screen, and sound.\"\n\"1\"\n\n\n\"I regret buying this laptop. It is too slow and the keyboard is too noisy\"\n\"2\"\n\n\n\"Not sure how to feel about my new washing machine. Great color, but hard to figure\"\n\"1\"\n\n\n\n\n\n\n\n\n\ncustom\nMallFrame.custom(col, prompt='', valid_resps='', pred_name='custom')\nProvide the full prompt that the LLM will process.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncol\nstr\nThe name of the text field to process\nrequired\n\n\nprompt\nstr\nThe prompt to send to the LLM along with the col\n''\n\n\npred_name\nstr\nA character vector with the name of the new column where the prediction will be placed\n'custom'\n\n\n\n\n\nExamples\n\nmy_prompt = (\n    \"Answer a question.\"\n    \"Return only the answer, no explanation\"\n    \"Acceptable answers are 'yes', 'no'\"\n    \"Answer this about the following text, is this a happy customer?:\"\n)\n\nreviews.llm.custom(\"review\", prompt = my_prompt)\n\n\n\n\n\n\nreview\ncustom\n\n\n\n\n\"This has been the best TV I've ever used. Great screen, and sound.\"\n\"Yes\"\n\n\n\"I regret buying this laptop. It is too slow and the keyboard is too noisy\"\n\"No\"\n\n\n\"Not sure how to feel about my new washing machine. Great color, but hard to figure\"\n\"No\"\n\n\n\n\n\n\n\n\n\nextract\nMallFrame.extract(col, labels='', expand_cols=False, additional='', pred_name='extract')\nPull a specific label from the text.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncol\nstr\nThe name of the text field to process\nrequired\n\n\nlabels\nlist\nA list or a DICT object that defines tells the LLM what to look for and return\n''\n\n\npred_name\nstr\nA character vector with the name of the new column where the prediction will be placed\n'extract'\n\n\nadditional\nstr\nInserts this text into the prompt sent to the LLM\n''\n\n\n\n\n\nExamples\n\n# Use 'labels' to let the function know what to extract\nreviews.llm.extract(\"review\", labels = \"product\")\n\n\n\n\n\n\nreview\nextract\n\n\n\n\n\"This has been the best TV I've ever used. Great screen, and sound.\"\n\"tv\"\n\n\n\"I regret buying this laptop. It is too slow and the keyboard is too noisy\"\n\"laptop\"\n\n\n\"Not sure how to feel about my new washing machine. Great color, but hard to figure\"\n\"washing machine\"\n\n\n\n\n\n\n\n# Use 'pred_name' to customize the new column's name\nreviews.llm.extract(\"review\", \"product\", pred_name = \"prod\")\n\n\n\n\n\n\nreview\nprod\n\n\n\n\n\"This has been the best TV I've ever used. Great screen, and sound.\"\n\"tv\"\n\n\n\"I regret buying this laptop. It is too slow and the keyboard is too noisy\"\n\"laptop\"\n\n\n\"Not sure how to feel about my new washing machine. Great color, but hard to figure\"\n\"washing machine\"\n\n\n\n\n\n\n\n# Pass a vector to request multiple things, the results will be pipe delimeted\n# in a single column\nreviews.llm.extract(\"review\", [\"product\", \"feelings\"])\n\n\n\n\n\n\nreview\nextract\n\n\n\n\n\"This has been the best TV I've ever used. Great screen, and sound.\"\n\"tv | great\"\n\n\n\"I regret buying this laptop. It is too slow and the keyboard is too noisy\"\n\"laptop|frustration\"\n\n\n\"Not sure how to feel about my new washing machine. Great color, but hard to figure\"\n\"washing machine | confusion\"\n\n\n\n\n\n\n\n# Set 'expand_cols' to True to split multiple lables\n# into individual columns\nreviews.llm.extract(\n    col=\"review\",\n    labels=[\"product\", \"feelings\"],\n    expand_cols=True\n    )\n\n\n\n\n\n\nreview\nproduct\nfeelings\n\n\n\n\n\"This has been the best TV I've ever used. Great screen, and sound.\"\n\"tv \"\n\" great\"\n\n\n\"I regret buying this laptop. It is too slow and the keyboard is too noisy\"\n\"laptop\"\n\"frustration\"\n\n\n\"Not sure how to feel about my new washing machine. Great color, but hard to figure\"\n\"washing machine \"\n\" confusion\"\n\n\n\n\n\n\n\n# Set custom names to the resulting columns\nreviews.llm.extract(\n    col=\"review\",\n    labels={\"prod\": \"product\", \"feels\": \"feelings\"},\n    expand_cols=True\n    )\n\n\n\n\n\n\nreview\nprod\nfeels\n\n\n\n\n\"This has been the best TV I've ever used. Great screen, and sound.\"\n\"tv \"\n\" great\"\n\n\n\"I regret buying this laptop. It is too slow and the keyboard is too noisy\"\n\"laptop\"\n\"frustration\"\n\n\n\"Not sure how to feel about my new washing machine. Great color, but hard to figure\"\n\"washing machine \"\n\" confusion\"\n\n\n\n\n\n\n\n\n\nsentiment\nMallFrame.sentiment(col, options=['positive', 'negative', 'neutral'], additional='', pred_name='sentiment')\nUse an LLM to run a sentiment analysis\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncol\nstr\nThe name of the text field to process\nrequired\n\n\noptions\nlist or dict\nA list of the sentiment options to use, or a named DICT object\n['positive', 'negative', 'neutral']\n\n\npred_name\nstr\nA character vector with the name of the new column where the prediction will be placed\n'sentiment'\n\n\nadditional\nstr\nInserts this text into the prompt sent to the LLM\n''\n\n\n\n\n\nExamples\n\nreviews.llm.sentiment(\"review\")\n\n\n\n\n\n\nreview\nsentiment\n\n\n\n\n\"This has been the best TV I've ever used. Great screen, and sound.\"\n\"positive\"\n\n\n\"I regret buying this laptop. It is too slow and the keyboard is too noisy\"\n\"negative\"\n\n\n\"Not sure how to feel about my new washing machine. Great color, but hard to figure\"\n\"neutral\"\n\n\n\n\n\n\n\n# Use 'pred_name' to customize the new column's name\nreviews.llm.sentiment(\"review\", pred_name=\"review_sentiment\")\n\n\n\n\n\n\nreview\nreview_sentiment\n\n\n\n\n\"This has been the best TV I've ever used. Great screen, and sound.\"\n\"positive\"\n\n\n\"I regret buying this laptop. It is too slow and the keyboard is too noisy\"\n\"negative\"\n\n\n\"Not sure how to feel about my new washing machine. Great color, but hard to figure\"\n\"neutral\"\n\n\n\n\n\n\n\n# Pass custom sentiment options\nreviews.llm.sentiment(\"review\", [\"positive\", \"negative\"])\n\n\n\n\n\n\nreview\nsentiment\n\n\n\n\n\"This has been the best TV I've ever used. Great screen, and sound.\"\n\"positive\"\n\n\n\"I regret buying this laptop. It is too slow and the keyboard is too noisy\"\n\"negative\"\n\n\n\"Not sure how to feel about my new washing machine. Great color, but hard to figure\"\n\"negative\"\n\n\n\n\n\n\n\n# Use a DICT object to specify values to return per sentiment\nreviews.llm.sentiment(\"review\", {\"positive\" : 1, \"negative\" : 0})\n\n\n\n\n\n\nreview\nsentiment\n\n\n\n\n\"This has been the best TV I've ever used. Great screen, and sound.\"\n1\n\n\n\"I regret buying this laptop. It is too slow and the keyboard is too noisy\"\n0\n\n\n\"Not sure how to feel about my new washing machine. Great color, but hard to figure\"\n0\n\n\n\n\n\n\n\n\n\nsummarize\nMallFrame.summarize(col, max_words=10, additional='', pred_name='summary')\nSummarize the text down to a specific number of words.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncol\nstr\nThe name of the text field to process\nrequired\n\n\nmax_words\nint\nMaximum number of words to use for the summary\n10\n\n\npred_name\nstr\nA character vector with the name of the new column where the prediction will be placed\n'summary'\n\n\nadditional\nstr\nInserts this text into the prompt sent to the LLM\n''\n\n\n\n\n\nExamples\n\n# Use max_words to set the maximum number of words to use for the summary\nreviews.llm.summarize(\"review\", max_words = 5)\n\n\n\n\n\n\nreview\nsummary\n\n\n\n\n\"This has been the best TV I've ever used. Great screen, and sound.\"\n\"great tv with good features\"\n\n\n\"I regret buying this laptop. It is too slow and the keyboard is too noisy\"\n\"laptop purchase was a mistake\"\n\n\n\"Not sure how to feel about my new washing machine. Great color, but hard to figure\"\n\"feeling uncertain about new purchase\"\n\n\n\n\n\n\n\n# Use 'pred_name' to customize the new column's name\nreviews.llm.summarize(\"review\", 5, pred_name = \"review_summary\")\n\n\n\n\n\n\nreview\nreview_summary\n\n\n\n\n\"This has been the best TV I've ever used. Great screen, and sound.\"\n\"great tv with good features\"\n\n\n\"I regret buying this laptop. It is too slow and the keyboard is too noisy\"\n\"laptop purchase was a mistake\"\n\n\n\"Not sure how to feel about my new washing machine. Great color, but hard to figure\"\n\"feeling uncertain about new purchase\"\n\n\n\n\n\n\n\n\n\ntranslate\nMallFrame.translate(col, language='', additional='', pred_name='translation')\nTranslate text into another language.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncol\nstr\nThe name of the text field to process\nrequired\n\n\nlanguage\nstr\nThe target language to translate to. For example ‘French’.\n''\n\n\npred_name\nstr\nA character vector with the name of the new column where the prediction will be placed\n'translation'\n\n\nadditional\nstr\nInserts this text into the prompt sent to the LLM\n''\n\n\n\n\n\nExamples\n\nreviews.llm.translate(\"review\", \"spanish\")\n\n\n\n\n\n\nreview\ntranslation\n\n\n\n\n\"This has been the best TV I've ever used. Great screen, and sound.\"\n\"Esta ha sido la mejor televisión que he utilizado hasta ahora. Gran pantalla y sonido.\"\n\n\n\"I regret buying this laptop. It is too slow and the keyboard is too noisy\"\n\"Me arrepiento de comprar este portátil. Es demasiado lento y la tecla es demasiado ruidosa.\"\n\n\n\"Not sure how to feel about my new washing machine. Great color, but hard to figure\"\n\"No estoy seguro de cómo sentirme con mi nueva lavadora. Un color maravilloso, pero muy difícil de en…\n\n\n\n\n\n\n\nreviews.llm.translate(\"review\", \"french\")\n\n\n\n\n\n\nreview\ntranslation\n\n\n\n\n\"This has been the best TV I've ever used. Great screen, and sound.\"\n\"Ceci était la meilleure télévision que j'ai jamais utilisée. Écran et son excellent.\"\n\n\n\"I regret buying this laptop. It is too slow and the keyboard is too noisy\"\n\"Je me regrette d'avoir acheté ce portable. Il est trop lent et le clavier fait trop de bruit.\"\n\n\n\"Not sure how to feel about my new washing machine. Great color, but hard to figure\"\n\"Je ne sais pas comment réagir à mon nouveau lave-linge. Couleur superbe, mais difficile à comprendre…\n\n\n\n\n\n\n\n\n\nuse\nMallFrame.use(backend='', model='', _cache='_mall_cache', **kwargs)\nDefine the model, backend, and other options to use to interact with the LLM.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbackend\nstr\nThe name of the backend to use. At the beginning of the session it defaults to “ollama”. If passing \"\", it will remain unchanged\n''\n\n\nmodel\nstr\nThe name of the model tha the backend should use. At the beginning of the session it defaults to “llama3.2”. If passing \"\", it will remain unchanged\n''\n\n\n_cache\nstr\nThe path of where to save the cached results. Passing \"\" disables the cache\n'_mall_cache'\n\n\n**kwargs\n\nArguments to pass to the downstream Python call. In this case, the chat function in ollama\n{}\n\n\n\n\n\nExamples\n\n# Additional arguments will be passed 'as-is' to the\n# downstream R function in this example, to ollama::chat()\nreviews.llm.use(\"ollama\", \"llama3.2\", seed = 100, temp = 0.1)\n\n{'backend': 'ollama',\n 'model': 'llama3.2',\n '_cache': '_mall_cache',\n 'options': {'seed': 100},\n 'seed': 100,\n 'temp': 0.1}\n\n\n\n# During the Python session, you can change any argument\n# individually and it will retain all of previous\n# arguments used\nreviews.llm.use(temp = 0.3)\n\n{'backend': 'ollama',\n 'model': 'llama3.2',\n '_cache': '_mall_cache',\n 'options': {'seed': 100},\n 'seed': 100,\n 'temp': 0.3}\n\n\n\n# Use _cache to modify the target folder for caching\nreviews.llm.use(_cache = \"_my_cache\")\n\n{'backend': 'ollama',\n 'model': 'llama3.2',\n '_cache': '_my_cache',\n 'options': {'seed': 100},\n 'seed': 100,\n 'temp': 0.3}\n\n\n\n# Leave _cache empty to turn off this functionality\nreviews.llm.use(_cache = \"\")\n\n{'backend': 'ollama',\n 'model': 'llama3.2',\n '_cache': '',\n 'options': {'seed': 100},\n 'seed': 100,\n 'temp': 0.3}\n\n\n\n\n\nverify\nMallFrame.verify(col, what='', yes_no=[1, 0], additional='', pred_name='verify')\nCheck to see if something is true about the text.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncol\nstr\nThe name of the text field to process\nrequired\n\n\nwhat\nstr\nThe statement or question that needs to be verified against the provided text\n''\n\n\nyes_no\nlist\nA positional list of size 2, which contains the values to return if true and false. The first position will be used as the ‘true’ value, and the second as the ‘false’ value\n[1, 0]\n\n\npred_name\nstr\nA character vector with the name of the new column where the prediction will be placed\n'verify'\n\n\nadditional\nstr\nInserts this text into the prompt sent to the LLM\n''\n\n\n\n\n\nExamples\n\nreviews.llm.verify(\"review\", \"is the customer happy\")\n\n\n\n\n\n\nreview\nverify\n\n\n\n\n\"This has been the best TV I've ever used. Great screen, and sound.\"\n1\n\n\n\"I regret buying this laptop. It is too slow and the keyboard is too noisy\"\n0\n\n\n\"Not sure how to feel about my new washing machine. Great color, but hard to figure\"\n0\n\n\n\n\n\n\n\n# Use 'yes_no' to modify the 'true' and 'false' values to return\nreviews.llm.verify(\"review\", \"is the customer happy\", [\"y\", \"n\"])\n\n\n\n\n\n\nreview\nverify\n\n\n\n\n\"This has been the best TV I've ever used. Great screen, and sound.\"\n\"y\"\n\n\n\"I regret buying this laptop. It is too slow and the keyboard is too noisy\"\n\"n\"\n\n\n\"Not sure how to feel about my new washing machine. Great color, but hard to figure\"\n\"n\""
  },
  {
    "objectID": "reference/r_index.html",
    "href": "reference/r_index.html",
    "title": "mall",
    "section": "",
    "text": "llm_classify() llm_vec_classify()\n      Categorize data as one of options given\nllm_custom() llm_vec_custom()\n      Send a custom prompt to the LLM\nllm_extract() llm_vec_extract()\n      Extract entities from text\nllm_sentiment() llm_vec_sentiment()\n      Sentiment analysis\nllm_summarize() llm_vec_summarize()\n      Summarize text\nllm_translate() llm_vec_translate()\n      Translates text to a specific language\nllm_use()\n      Specify the model to use\nllm_verify() llm_vec_verify()\n      Verify if a statement about the text is true or not\nreviews\n      Mini reviews data set"
  },
  {
    "objectID": "reference/llm_custom.html",
    "href": "reference/llm_custom.html",
    "title": "Send a custom prompt to the LLM",
    "section": "",
    "text": "R/llm-custom.R"
  },
  {
    "objectID": "reference/llm_custom.html#llm_custom",
    "href": "reference/llm_custom.html#llm_custom",
    "title": "Send a custom prompt to the LLM",
    "section": "llm_custom",
    "text": "llm_custom"
  },
  {
    "objectID": "reference/llm_custom.html#description",
    "href": "reference/llm_custom.html#description",
    "title": "Send a custom prompt to the LLM",
    "section": "Description",
    "text": "Description\nUse a Large Language Model (LLM) to process the provided text using the instructions from prompt"
  },
  {
    "objectID": "reference/llm_custom.html#usage",
    "href": "reference/llm_custom.html#usage",
    "title": "Send a custom prompt to the LLM",
    "section": "Usage",
    "text": "Usage\n \nllm_custom(.data, col, prompt = \"\", pred_name = \".pred\", valid_resps = \"\") \n \nllm_vec_custom(x, prompt = \"\", valid_resps = NULL)"
  },
  {
    "objectID": "reference/llm_custom.html#arguments",
    "href": "reference/llm_custom.html#arguments",
    "title": "Send a custom prompt to the LLM",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\n.data\nA data.frame or tbl object that contains the text to be analyzed\n\n\ncol\nThe name of the field to analyze, supports tidy-eval\n\n\nprompt\nThe prompt to append to each record sent to the LLM\n\n\npred_name\nA character vector with the name of the new column where the prediction will be placed\n\n\nvalid_resps\nIf the response from the LLM is not open, but deterministic, provide the options in a vector. This function will set to NA any response not in the options\n\n\nx\nA vector that contains the text to be analyzed"
  },
  {
    "objectID": "reference/llm_custom.html#value",
    "href": "reference/llm_custom.html#value",
    "title": "Send a custom prompt to the LLM",
    "section": "Value",
    "text": "Value\nllm_custom returns a data.frame or tbl object. llm_vec_custom returns a vector that is the same length as x."
  },
  {
    "objectID": "reference/llm_custom.html#examples",
    "href": "reference/llm_custom.html#examples",
    "title": "Send a custom prompt to the LLM",
    "section": "Examples",
    "text": "Examples\n\n \nlibrary(mall) \n \ndata(\"reviews\") \n \nllm_use(\"ollama\", \"llama3.2\", seed = 100, .silent = TRUE) \n \nmy_prompt &lt;- paste( \n  \"Answer a question.\", \n  \"Return only the answer, no explanation\", \n  \"Acceptable answers are 'yes', 'no'\", \n  \"Answer this about the following text, is this a happy customer?:\" \n) \n \nreviews |&gt; \n  llm_custom(review, my_prompt) \n#&gt; # A tibble: 3 × 2\n#&gt;   review                                                                   .pred\n#&gt;   &lt;chr&gt;                                                                    &lt;chr&gt;\n#&gt; 1 This has been the best TV I've ever used. Great screen, and sound.       Yes  \n#&gt; 2 I regret buying this laptop. It is too slow and the keyboard is too noi… No   \n#&gt; 3 Not sure how to feel about my new washing machine. Great color, but har… No"
  },
  {
    "objectID": "reference/llm_verify.html",
    "href": "reference/llm_verify.html",
    "title": "Verify if a statement about the text is true or not",
    "section": "",
    "text": "R/llm-verify.R"
  },
  {
    "objectID": "reference/llm_verify.html#llm_verify",
    "href": "reference/llm_verify.html#llm_verify",
    "title": "Verify if a statement about the text is true or not",
    "section": "llm_verify",
    "text": "llm_verify"
  },
  {
    "objectID": "reference/llm_verify.html#description",
    "href": "reference/llm_verify.html#description",
    "title": "Verify if a statement about the text is true or not",
    "section": "Description",
    "text": "Description\nUse a Large Language Model (LLM) to see if something is true or not based the provided text"
  },
  {
    "objectID": "reference/llm_verify.html#usage",
    "href": "reference/llm_verify.html#usage",
    "title": "Verify if a statement about the text is true or not",
    "section": "Usage",
    "text": "Usage\n \nllm_verify( \n  .data, \n  col, \n  what, \n  yes_no = factor(c(1, 0)), \n  pred_name = \".verify\", \n  additional_prompt = \"\" \n) \n \nllm_vec_verify( \n  x, \n  what, \n  yes_no = factor(c(1, 0)), \n  additional_prompt = \"\", \n  preview = FALSE \n)"
  },
  {
    "objectID": "reference/llm_verify.html#arguments",
    "href": "reference/llm_verify.html#arguments",
    "title": "Verify if a statement about the text is true or not",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\n.data\nA data.frame or tbl object that contains the text to be analyzed\n\n\ncol\nThe name of the field to analyze, supports tidy-eval\n\n\nwhat\nThe statement or question that needs to be verified against the provided text\n\n\nyes_no\nA size 2 vector that specifies the expected output. It is positional. The first item is expected to be value to return if the statement about the provided text is true, and the second if it is not. Defaults to: factor(c(1, 0))\n\n\npred_name\nA character vector with the name of the new column where the prediction will be placed\n\n\nadditional_prompt\nInserts this text into the prompt sent to the LLM\n\n\nx\nA vector that contains the text to be analyzed\n\n\npreview\nIt returns the R call that would have been used to run the prediction. It only returns the first record in x. Defaults to FALSE Applies to vector function only."
  },
  {
    "objectID": "reference/llm_verify.html#value",
    "href": "reference/llm_verify.html#value",
    "title": "Verify if a statement about the text is true or not",
    "section": "Value",
    "text": "Value\nllm_verify returns a data.frame or tbl object. llm_vec_verify returns a vector that is the same length as x."
  },
  {
    "objectID": "reference/llm_verify.html#examples",
    "href": "reference/llm_verify.html#examples",
    "title": "Verify if a statement about the text is true or not",
    "section": "Examples",
    "text": "Examples\n\n \nlibrary(mall) \n \ndata(\"reviews\") \n \nllm_use(\"ollama\", \"llama3.2\", seed = 100, .silent = TRUE) \n \n# By default it will return 1 for 'true', and 0 for 'false', \n# the new column will be a factor type \nllm_verify(reviews, review, \"is the customer happy\") \n#&gt; # A tibble: 3 × 2\n#&gt;   review                                                                 .verify\n#&gt;   &lt;chr&gt;                                                                  &lt;fct&gt;  \n#&gt; 1 This has been the best TV I've ever used. Great screen, and sound.     1      \n#&gt; 2 I regret buying this laptop. It is too slow and the keyboard is too n… 0      \n#&gt; 3 Not sure how to feel about my new washing machine. Great color, but h… 0\n \n# The yes_no argument can be modified to return a different response \n# than 1 or 0. First position will be 'true' and second, 'false' \nllm_verify(reviews, review, \"is the customer happy\", c(\"y\", \"n\")) \n#&gt; # A tibble: 3 × 2\n#&gt;   review                                                                 .verify\n#&gt;   &lt;chr&gt;                                                                  &lt;chr&gt;  \n#&gt; 1 This has been the best TV I've ever used. Great screen, and sound.     y      \n#&gt; 2 I regret buying this laptop. It is too slow and the keyboard is too n… n      \n#&gt; 3 Not sure how to feel about my new washing machine. Great color, but h… n\n \n# Number can also be used, this would be in the case that you wish to match \n# the output values of existing predictions \nllm_verify(reviews, review, \"is the customer happy\", c(2, 1)) \n#&gt; # A tibble: 3 × 2\n#&gt;   review                                                                 .verify\n#&gt;   &lt;chr&gt;                                                                    &lt;dbl&gt;\n#&gt; 1 This has been the best TV I've ever used. Great screen, and sound.           2\n#&gt; 2 I regret buying this laptop. It is too slow and the keyboard is too n…       1\n#&gt; 3 Not sure how to feel about my new washing machine. Great color, but h…       1"
  },
  {
    "objectID": "reference/llm_translate.html",
    "href": "reference/llm_translate.html",
    "title": "Translates text to a specific language",
    "section": "",
    "text": "R/llm-translate.R"
  },
  {
    "objectID": "reference/llm_translate.html#llm_translate",
    "href": "reference/llm_translate.html#llm_translate",
    "title": "Translates text to a specific language",
    "section": "llm_translate",
    "text": "llm_translate"
  },
  {
    "objectID": "reference/llm_translate.html#description",
    "href": "reference/llm_translate.html#description",
    "title": "Translates text to a specific language",
    "section": "Description",
    "text": "Description\nUse a Large Language Model (LLM) to translate a text to a specific language"
  },
  {
    "objectID": "reference/llm_translate.html#usage",
    "href": "reference/llm_translate.html#usage",
    "title": "Translates text to a specific language",
    "section": "Usage",
    "text": "Usage\n \nllm_translate( \n  .data, \n  col, \n  language, \n  pred_name = \".translation\", \n  additional_prompt = \"\" \n) \n \nllm_vec_translate(x, language, additional_prompt = \"\", preview = FALSE)"
  },
  {
    "objectID": "reference/llm_translate.html#arguments",
    "href": "reference/llm_translate.html#arguments",
    "title": "Translates text to a specific language",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\n.data\nA data.frame or tbl object that contains the text to be analyzed\n\n\ncol\nThe name of the field to analyze, supports tidy-eval\n\n\nlanguage\nTarget language to translate the text to\n\n\npred_name\nA character vector with the name of the new column where the prediction will be placed\n\n\nadditional_prompt\nInserts this text into the prompt sent to the LLM\n\n\nx\nA vector that contains the text to be analyzed\n\n\npreview\nIt returns the R call that would have been used to run the prediction. It only returns the first record in x. Defaults to FALSE Applies to vector function only."
  },
  {
    "objectID": "reference/llm_translate.html#value",
    "href": "reference/llm_translate.html#value",
    "title": "Translates text to a specific language",
    "section": "Value",
    "text": "Value\nllm_translate returns a data.frame or tbl object. llm_vec_translate returns a vector that is the same length as x."
  },
  {
    "objectID": "reference/llm_translate.html#examples",
    "href": "reference/llm_translate.html#examples",
    "title": "Translates text to a specific language",
    "section": "Examples",
    "text": "Examples\n\n \nlibrary(mall) \n \ndata(\"reviews\") \n \nllm_use(\"ollama\", \"llama3.2\", seed = 100, .silent = TRUE) \n \n# Pass the desired language to translate to \nllm_translate(reviews, review, \"spanish\") \n#&gt; # A tibble: 3 × 2\n#&gt;   review                                        .translation                    \n#&gt;   &lt;chr&gt;                                         &lt;chr&gt;                           \n#&gt; 1 This has been the best TV I've ever used. Gr… Esta ha sido la mejor televisió…\n#&gt; 2 I regret buying this laptop. It is too slow … Me arrepiento de comprar este p…\n#&gt; 3 Not sure how to feel about my new washing ma… No estoy seguro de cómo me sien…"
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "mall",
    "section": "",
    "text": "RPython\n\n\n\nllm_classify() llm_vec_classify()\n      Categorize data as one of options given\nllm_custom() llm_vec_custom()\n      Send a custom prompt to the LLM\nllm_extract() llm_vec_extract()\n      Extract entities from text\nllm_sentiment() llm_vec_sentiment()\n      Sentiment analysis\nllm_summarize() llm_vec_summarize()\n      Summarize text\nllm_translate() llm_vec_translate()\n      Translates text to a specific language\nllm_use()\n      Specify the model to use\nllm_verify() llm_vec_verify()\n      Verify if a statement about the text is true or not\nreviews\n      Mini reviews data set\n\n\n\nMallFrame\n      Extension to Polars that add ability to use an LLM to run batch predictions over a data frame\n\n\n\nName\nDescription\n\n\n\n\nclassify\nClassify text into specific categories.\n\n\ncustom\nProvide the full prompt that the LLM will process.\n\n\nextract\nPull a specific label from the text.\n\n\nsentiment\nUse an LLM to run a sentiment analysis\n\n\nsummarize\nSummarize the text down to a specific number of words.\n\n\ntranslate\nTranslate text into another language.\n\n\nuse\nDefine the model, backend, and other options to use to\n\n\nverify\nCheck to see if something is true about the text."
  }
]