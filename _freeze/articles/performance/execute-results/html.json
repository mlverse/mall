{
  "hash": "725f7d35a501e3cb52a4e8704cdb7a78",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Performance\"\nexecute:\n  eval: true\n  freeze: true\n---\n\n\n\n\n\n\n## Performance\n\nWe will briefly cover this methods performance from two perspectives: \n\n- How long the analysis takes to run locally \n\n- How well it predicts \n\nTo do so, we will use the `data_bookReviews` data set, provided by the `classmap`\npackage. For this exercise, only the first 100, of the total 1,000, are going\nto be part of this analysis.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mall)\nlibrary(classmap)\nlibrary(dplyr)\n\ndata(data_bookReviews)\n\ndata_bookReviews |>\n  glimpse()\n#> Rows: 1,000\n#> Columns: 2\n#> $ review    <chr> \"i got this as both a book and an audio file. i had waited t…\n#> $ sentiment <fct> 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, …\n```\n:::\n\n\n\nAs per the docs, `sentiment` is a factor indicating the sentiment of the review:\nnegative (1) or positive (2)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlength(strsplit(paste(head(data_bookReviews$review, 100), collapse = \" \"), \" \")[[1]])\n#> [1] 20470\n```\n:::\n\n\n\n\nJust to get an idea of how much data we're processing, I'm using a very, very \nsimple word count. So we're analyzing a bit over 20 thousand words.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreviews_llm <- data_bookReviews |>\n  head(100) |> \n  llm_sentiment(\n    col = review,\n    options = c(\"positive\" ~ 2, \"negative\" ~ 1),\n    pred_name = \"predicted\"\n  )\n#> ! There were 2 predictions with invalid output, they were coerced to NA\n```\n:::\n\n\n\n\nAs far as **time**, on my Apple M3 machine, it took about 1.5 minutes to process,\n100 rows, containing 20 thousand words. Setting `temp` to 0 in `llm_use()`, \nmade the model run faster.\n\nThe package uses `purrr` to send each prompt individually to the LLM. But, I did\ntry a few different ways to speed up the process, unsuccessfully:\n\n- Used `furrr` to send multiple requests at a time. This did not work because \neither the LLM or Ollama processed all my requests serially. So there was\nno improvement.\n\n- I also tried sending more than one row's text at a time. This cause instability\nin the number of results. For example sending 5 at a time, sometimes returned 7\nor 8. Even sending 2 was not stable. \n\nThis is what the new table looks like:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreviews_llm\n#> # A tibble: 100 × 3\n#>    review                                        sentiment             predicted\n#>    <chr>                                         <fct>                     <dbl>\n#>  1 \"i got this as both a book and an audio file… 1                             1\n#>  2 \"this book places too much emphasis on spend… 1                             1\n#>  3 \"remember the hollywood blacklist? the holly… 2                             2\n#>  4 \"while i appreciate what tipler was attempti… 1                             1\n#>  5 \"the others in the series were great, and i … 1                             1\n#>  6 \"a few good things, but she's lost her edge … 1                             1\n#>  7 \"words cannot describe how ripped off and di… 1                             1\n#>  8 \"1. the persective of most writers is shaped… 1                            NA\n#>  9 \"i have been a huge fan of michael crichton … 1                             1\n#> 10 \"i saw dr. polk on c-span a month or two ago… 2                             2\n#> # ℹ 90 more rows\n```\n:::\n\n\n\n\nI used `yardstick` to see how well the model performed. Of course, the accuracy\nwill not be of the \"truth\", but rather the package's results recorded in \n`sentiment`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(forcats)\n\nreviews_llm |>\n  mutate(predicted = as.factor(predicted)) |>\n  yardstick::accuracy(sentiment, predicted)\n#> # A tibble: 1 × 3\n#>   .metric  .estimator .estimate\n#>   <chr>    <chr>          <dbl>\n#> 1 accuracy binary         0.980\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}