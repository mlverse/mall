{
  "hash": "ab023ee0c0195b9d05c7ad72ea6b5203",
  "result": {
    "engine": "knitr",
    "markdown": "---\nformat:\n  html:\n    toc: true\nexecute:\n  eval: true\n  freeze: true\n---\n\n\n\n\n\n<img src=\"site/images/favicon/apple-touch-icon-180x180.png\" style=\"float:right\"/>\n\n<!-- badges: start -->\n\n[![PyPi](https://img.shields.io/pypi/v/mlverse-mall)](https://pypi.org/project/mlverse-mall/) [![Python tests](https://github.com/mlverse/mall/actions/workflows/python-tests.yaml/badge.svg)](https://github.com/mlverse/mall/actions/workflows/python-tests.yaml) \\| <a href=\"https://cran.r-project.org/package=mall\" data-original-href=\"https://cran.r-project.org/package=mall\"><img src=\"https://www.r-pkg.org/badges/version/mall\" alt=\"CRAN status\" class=\"img-fluid\"/></a> [![R check](https://github.com/mlverse/mall/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/mlverse/mall/actions/workflows/R-CMD-check.yaml) \\| [![Package coverage](https://codecov.io/gh/mlverse/mall/branch/main/graph/badge.svg)](https://app.codecov.io/gh/mlverse/mall?branch=main)\n\n<!-- badges: end -->\n\nUse Large Language Models (LLM) to run Natural Language Processing (NLP) \noperations against your data. It takes advantage of the LLMs general language\ntraining in order to get the predictions, thus removing the need to train a new\nNLP model. `mall` is available for R and Python.\n\nIt works by running multiple LLM predictions against your data. The predictions\nare processed row-wise over a specified column. It relies on the \"one-shot\" \nprompt technique to instruct the LLM on a particular NLP operation to perform. \nThe package includes prompts to perform the following specific NLP operations:\n\n-   [Sentiment analysis](#sentiment)\n-   [Text summarizing](#summarize)\n-   [Classify text](#classify)\n-   [Extract one, or several](#extract), specific pieces information from the text\n-   [Translate text](#translate)\n-   [Verify that something is true](#verify) about the text (binary)\n\nFor other NLP operations, `mall` offers the ability for you to [write your own prompt](#custom-prompt).\n\n<img src=\"site/images/dplyr.png\" style=\"float:left;margin-right:20px;margin-top:22px;\" width=\"85\"/>\n\nIn **R** The functions inside `mall` are designed to easily work with piped \ncommands, such as `dplyr`.\n\n``` r\nreviews |>\n  llm_sentiment(review)\n```\n\n<img src=\"site/images/polars.png\" style=\"float:left;margin-right:10px;\" width=\"95\"/>\n\nIn **Python**, `mall` is a library extension to [Polars](https://pola.rs/).\n\n``` python\nreviews.llm.sentiment(\"review\")\n```\n\n## Motivation\n\nWe want to new find new ways to help data scientists use LLMs in their daily work.\nUnlike the familiar interfaces, such as chatting and code completion, this \ninterface runs your text data directly against the LLM. This package is inspired\nby the SQL AI functions now offered by vendors such as [Databricks](https://docs.databricks.com/en/large-language-models/ai-functions.html) \nand Snowflake. \n\nThe LLM's flexibility, allows for it to adapt to the subject of your data, and\nprovide surprisingly accurate predictions. This saves the data scientist the \nneed to write and tune an NLP model.\n\nIn recent times, the capabilities of LLMs that can run locally in your computer \nhave increased dramatically.  This means that these sort of analysis can run in \nyour machine with good accuracy. It also makes it possible to take \nadvantage of LLMs at your institution, since the data will not leave the \ncorporate network. Additionally, LLM management and integration platforms, such\nas [Ollama](https://ollama.com/), are now very easy to setup and use. `mall`\nuses Ollama as to interact with local LLMs.\n\nThe development version of `mall` lets you **use external LLMs such as\n[OpenAI](https://openai.com/), [Gemini](https://gemini.google.com/) and\n[Anthropic](https://www.anthropic.com/)**. In R, `mall` uses the\n[`ellmer`](https://ellmer.tidyverse.org/index.html)\npackage to integrate with the external LLM, and the \n[`chatlas`](https://posit-dev.github.io/chatlas/) package to integrate in Python.\n\n## Install `mall` {#get-started}\n\nInstall the package to get started:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nOfficial version from CRAN:\n\n``` r\ninstall.packages(\"mall\")\n```\n\nDevelopment version from GitHub *(required for remote LLM integration)*:\n\n``` r\npak::pak(\"mlverse/mall/r\")\n```\n\n## Python\n\nOfficial version from PyPi:\n\n``` python\npip install mlverse-mall\n```\n\nDevelopment version from GitHub:\n\n``` python\npip install \"mlverse-mall @ git+https://git@github.com/mlverse/mall.git#subdirectory=python\"\n```\n:::\n\n## Setup the LLM\n\nChoose one of the two following options to setup LLM connectivity:\n\n### Local LLMs, via Ollama {#local-llms}\n\n-   [Download Ollama from the official website](https://ollama.com/download)\n\n-   Install and start Ollama in your computer\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n-   Install Ollama in your machine. The `ollamar` package's website provides this \n[Installation guide](https://hauselin.github.io/ollama-r/#installation)\n\n-   Download an LLM model. For example, I have been developing this package using \nLlama 3.2 to test. To get that model you can run:\n\n    ``` r\n    ollamar::pull(\"llama3.2\")\n    ```\n\n## Python\n\n-   Install the official Ollama library\n\n    ``` python\n    pip install ollama\n    ```\n\n-   Download an LLM model. For example, I have been developing this package\nusing Llama 3.2 to test. To get that model you can run:\n\n    ``` python\n    import ollama\n    ollama.pull('llama3.2')\n    ```\n:::\n\n### Remote LLMs {#remote-llms}\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n`mall` uses the `ellmer` package as the integration point to the LLM. This package supports multiple providers such as OpenAI, Anthropic, Google Gemini, etc.\n\n-   Install `ellmer`\n\n    ``` r\n    install.packages(\"ellmer\")\n    ```\n\n-   Refer to `ellmer`'s documentation to find out how to setup the connections with your selected provider: <https://ellmer.tidyverse.org/reference/index.html#chatbots>\n\n-   Let `mall` know which `ellmer` object to use during the R session. To do this, call `llm_use()`. Here is an example of using OpenAI:\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    library(mall)\n    #> Warning: replacing previous import 'ellmer::chat' by 'ollamar::chat' when\n    #> loading 'mall'\n    library(ellmer)\n    chat <- chat_openai()\n    #> Using model = \"gpt-4.1\".\n    llm_use(chat)\n    #> \n    #> ── mall session object \n    #> Backend: ellmerLLM session: model:gpt-4.1R session:\n    #> cache_folder:/var/folders/y_/f_0cx_291nl0s8h26t4jg6ch0000gp/T//RtmpQ0xozP/_mall_cachebfea6383a33a\n    ```\n    :::\n\n\n**Set a default LLM for your R session**\n\nAs a convenience, `mall` is able to automatically establish a connection with the\nLLM at the beginning o R session. To do this you can use the `.mall_chat` option:\n\n```r\noptions(.mall_chat =  ellmer::chat_openai(model = \"gpt-4o\"))\n```\n\nAdd this line to your *.Rprofile* file in order for that code to run every time\nyou start R.  You can call `usethis::edit_r_profile()` to open your .Rprofile\nfile so you can add the option. \n\n## Python\n\n`mall` uses the `chatlas` package as the integration point to the LLM. This \npackage supports multiple providers such as OpenAI, Anthropic, Google Gemini, etc.\n\n-   Install the `chatlas` library\n\n    ``` python\n    pip install chatlas\n    ```\n\n-   Refer to `chatlas`'s documentation to find out how to setup the connections\nwith your selected provider: <https://posit-dev.github.io/chatlas/reference/#chat-model-providers>\n\n-   Let `mall` know which `chatlas` object to use during the Python session. \nTo do this, call `llm_use()`. Here is an example of using OpenAI:\n\n    ``` python\n    import mall\n    from chatlas import ChatOpenAI\n\n    chat = ChatOpenAI()\n\n    data = mall.MallData\n    reviews = data.reviews\n\n    reviews.llm.use(chat)\n    ```\n:::\n\n## LLM functions\n\nWe will start with loading a very small data set contained in `mall`. It has \n3 product reviews that we will use as the source of our examples.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mall)\ndata(\"reviews\")\n\nreviews\n#> # A tibble: 3 × 1\n#>   review                                                                        \n#>   <chr>                                                                         \n#> 1 This has been the best TV I've ever used. Great screen, and sound.            \n#> 2 I regret buying this laptop. It is too slow and the keyboard is too noisy     \n#> 3 Not sure how to feel about my new washing machine. Great color, but hard to f…\n```\n:::\n\n\n## Python\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport mall \ndata = mall.MallData\nreviews = data.reviews\n\nreviews \n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<table border=\"1\" class=\"dataframe\"><thead><tr><th>review</th></tr></thead><tbody><tr><td>&quot;This has been the best TV I&#x27;ve ever used. Great screen, and sound.&quot;</td></tr><tr><td>&quot;I regret buying this laptop. It is too slow and the keyboard is too noisy&quot;</td></tr><tr><td>&quot;Not sure how to feel about my new washing machine. Great color, but hard to figure&quot;</td></tr></tbody></table></div>\n```\n\n:::\n:::\n\n:::\n\n### Sentiment {#sentiment}\n\nAutomatically returns \"positive\", \"negative\", or \"neutral\" based on the text.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreviews |>\n  llm_sentiment(review)\n#> # A tibble: 3 × 2\n#>   review                                                              .sentiment\n#>   <chr>                                                               <chr>     \n#> 1 This has been the best TV I've ever used. Great screen, and sound.  positive  \n#> 2 I regret buying this laptop. It is too slow and the keyboard is to… negative  \n#> 3 Not sure how to feel about my new washing machine. Great color, bu… neutral\n```\n:::\n\n\nFor more information and examples visit this function's [R reference page](reference/llm_sentiment.qmd)\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nreviews.llm.sentiment(\"review\")\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<table border=\"1\" class=\"dataframe\"><thead><tr><th>review</th><th>sentiment</th></tr></thead><tbody><tr><td>&quot;This has been the best TV I&#x27;ve ever used. Great screen, and sound.&quot;</td><td>&quot;positive&quot;</td></tr><tr><td>&quot;I regret buying this laptop. It is too slow and the keyboard is too noisy&quot;</td><td>&quot;negative&quot;</td></tr><tr><td>&quot;Not sure how to feel about my new washing machine. Great color, but hard to figure&quot;</td><td>&quot;negative&quot;</td></tr></tbody></table></div>\n```\n\n:::\n:::\n\n\nFor more information and examples visit this function's [Python reference page](reference/MallFrame.qmd#mall.MallFrame.sentiment)\n:::\n\n### Summarize {#summarize}\n\nThere may be a need to reduce the number of words in a given text. Typically to \nmake it easier to understand its intent. The function has an argument to control \nthe maximum number of words to output (`max_words`):\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreviews |>\n  llm_summarize(review, max_words = 5)\n#> # A tibble: 3 × 2\n#>   review                                        .summary                      \n#>   <chr>                                         <chr>                         \n#> 1 This has been the best TV I've ever used. Gr… great tv with good features   \n#> 2 I regret buying this laptop. It is too slow … laptop purchase was a mistake \n#> 3 Not sure how to feel about my new washing ma… having mixed feelings about it\n```\n:::\n\n\nFor more information and examples visit this function's [R reference page](reference/llm_summarize.qmd)\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nreviews.llm.summarize(\"review\", 5)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<table border=\"1\" class=\"dataframe\"><thead><tr><th>review</th><th>summary</th></tr></thead><tbody><tr><td>&quot;This has been the best TV I&#x27;ve ever used. Great screen, and sound.&quot;</td><td>&quot;best tv ever purchased&quot;</td></tr><tr><td>&quot;I regret buying this laptop. It is too slow and the keyboard is too noisy&quot;</td><td>&quot;laptop not up to expectations&quot;</td></tr><tr><td>&quot;Not sure how to feel about my new washing machine. Great color, but hard to figure&quot;</td><td>&quot;uncertain about new washer&quot;</td></tr></tbody></table></div>\n```\n\n:::\n:::\n\n\nFor more information and examples visit this function's [Python reference page](reference/MallFrame.qmd#mall.MallFrame.summarize)\n:::\n\n### Classify {#classify}\n\nUse the LLM to categorize the text into one of the options you provide:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreviews |>\n  llm_classify(review, c(\"appliance\", \"computer\"))\n#> # A tibble: 3 × 2\n#>   review                                        .classify\n#>   <chr>                                         <chr>    \n#> 1 This has been the best TV I've ever used. Gr… computer \n#> 2 I regret buying this laptop. It is too slow … computer \n#> 3 Not sure how to feel about my new washing ma… appliance\n```\n:::\n\n\nFor more information and examples visit this function's [R reference page](reference/llm_classify.qmd)\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nreviews.llm.classify(\"review\", [\"computer\", \"appliance\"])\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<table border=\"1\" class=\"dataframe\"><thead><tr><th>review</th><th>classify</th></tr></thead><tbody><tr><td>&quot;This has been the best TV I&#x27;ve ever used. Great screen, and sound.&quot;</td><td>&quot;appliance&quot;</td></tr><tr><td>&quot;I regret buying this laptop. It is too slow and the keyboard is too noisy&quot;</td><td>&quot;appliance&quot;</td></tr><tr><td>&quot;Not sure how to feel about my new washing machine. Great color, but hard to figure&quot;</td><td>&quot;appliance&quot;</td></tr></tbody></table></div>\n```\n\n:::\n:::\n\n\nFor more information and examples visit this function's [Python reference page](reference/MallFrame.qmd#mall.MallFrame.classify)\n:::\n\n### Extract {#extract}\n\nOne of the most interesting use cases Using natural language, we can tell the \nLLM to return a specific part of the text. In the following example, we request \nthat the LLM return the product being referred to. We do this by simply saying \n\"product\". The LLM understands what we *mean* by that word, and looks for that \nin the text.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreviews |>\n  llm_extract(review, \"product\")\n#> # A tibble: 3 × 2\n#>   review                                        .extract       \n#>   <chr>                                         <chr>          \n#> 1 This has been the best TV I've ever used. Gr… tv             \n#> 2 I regret buying this laptop. It is too slow … laptop         \n#> 3 Not sure how to feel about my new washing ma… washing machine\n```\n:::\n\n\nFor more information and examples visit this function's [R reference page](reference/llm_extract.qmd)\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nreviews.llm.extract(\"review\", \"product\")\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<table border=\"1\" class=\"dataframe\"><thead><tr><th>review</th><th>extract</th></tr></thead><tbody><tr><td>&quot;This has been the best TV I&#x27;ve ever used. Great screen, and sound.&quot;</td><td>&quot;tv&quot;</td></tr><tr><td>&quot;I regret buying this laptop. It is too slow and the keyboard is too noisy&quot;</td><td>&quot;laptop&quot;</td></tr><tr><td>&quot;Not sure how to feel about my new washing machine. Great color, but hard to figure&quot;</td><td>&quot;washing machine&quot;</td></tr></tbody></table></div>\n```\n\n:::\n:::\n\n\nFor more information and examples visit this function's [Python reference page](reference/MallFrame.qmd#mall.MallFrame.extract)\n:::\n\n### Verify {#verify}\n\nThis functions allows you to check and see if a statement is true, based on the\nprovided text. By default, it will return a 1 for \"yes\", and 0 for \"no\". This \ncan be customized.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreviews |>\n  llm_verify(review, \"is the customer happy with the purchase\")\n#> # A tibble: 3 × 2\n#>   review                                                                 .verify\n#>   <chr>                                                                  <fct>  \n#> 1 This has been the best TV I've ever used. Great screen, and sound.     1      \n#> 2 I regret buying this laptop. It is too slow and the keyboard is too n… 0      \n#> 3 Not sure how to feel about my new washing machine. Great color, but h… 0\n```\n:::\n\n\nFor more information and examples visit this function's [R reference page](reference/llm_verify.qmd)\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nreviews.llm.verify(\"review\", \"is the customer happy with the purchase\")\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<table border=\"1\" class=\"dataframe\"><thead><tr><th>review</th><th>verify</th></tr></thead><tbody><tr><td>&quot;This has been the best TV I&#x27;ve ever used. Great screen, and sound.&quot;</td><td>1</td></tr><tr><td>&quot;I regret buying this laptop. It is too slow and the keyboard is too noisy&quot;</td><td>0</td></tr><tr><td>&quot;Not sure how to feel about my new washing machine. Great color, but hard to figure&quot;</td><td>0</td></tr></tbody></table></div>\n```\n\n:::\n:::\n\n\nFor more information and examples visit this function's [Python reference page](reference/MallFrame.qmd#mall.MallFrame.verify)\n:::\n\n### Translate {#translate}\n\nAs the title implies, this function will translate the text into a specified \nlanguage. What is really nice, it is that you don't need to specify the language\nof the source text. Only the target language needs to be defined. The \ntranslation accuracy will depend on the LLM\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreviews |>\n  llm_translate(review, \"spanish\")\n#> # A tibble: 3 × 2\n#>   review                                        .translation                    \n#>   <chr>                                         <chr>                           \n#> 1 This has been the best TV I've ever used. Gr… Esta ha sido la mejor televisió…\n#> 2 I regret buying this laptop. It is too slow … Me arrepiento de comprar este p…\n#> 3 Not sure how to feel about my new washing ma… No estoy seguro de cómo me sien…\n```\n:::\n\n\nFor more information and examples visit this function's [R reference page](reference/llm_translate.qmd)\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nreviews.llm.translate(\"review\", \"spanish\")\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<table border=\"1\" class=\"dataframe\"><thead><tr><th>review</th><th>translation</th></tr></thead><tbody><tr><td>&quot;This has been the best TV I&#x27;ve ever used. Great screen, and sound.&quot;</td><td>&quot;Esta ha sido la mejor TV que he utilizado. Gran pantalla y sonido.&quot;</td></tr><tr><td>&quot;I regret buying this laptop. It is too slow and the keyboard is too noisy&quot;</td><td>&quot;Lamento haber comprado este portátil. Está demasiado lento y la tecla es demasiado ruidosa.&quot;</td></tr><tr><td>&quot;Not sure how to feel about my new washing machine. Great color, but hard to figure&quot;</td><td>&quot;No estoy seguro de cómo sentirme con mi nueva lavadora. Bonito color, pero difícil de entender.&quot;</td></tr></tbody></table></div>\n```\n\n:::\n:::\n\n\nFor more information and examples visit this function's [Python reference page](reference/MallFrame.qmd#mall.MallFrame.translate)\n:::\n\n### Custom prompt {#custom-prompt}\n\nIt is possible to pass your own prompt to the LLM, and have `mall` run it \nagainst each text entry:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_prompt <- paste(\n  \"Answer a question.\",\n  \"Return only the answer, no explanation\",\n  \"Acceptable answers are 'yes', 'no'\",\n  \"Answer this about the following text, is this a happy customer?:\"\n)\n\nreviews |>\n  llm_custom(review, my_prompt)\n#> # A tibble: 3 × 2\n#>   review                                                                   .pred\n#>   <chr>                                                                    <chr>\n#> 1 This has been the best TV I've ever used. Great screen, and sound.       Yes  \n#> 2 I regret buying this laptop. It is too slow and the keyboard is too noi… No   \n#> 3 Not sure how to feel about my new washing machine. Great color, but har… No\n```\n:::\n\n\nFor more information and examples visit this function's [R reference page](reference/llm_custom.qmd)\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmy_prompt = (\n    \"Answer a question.\"\n    \"Return only the answer, no explanation\"\n    \"Acceptable answers are 'yes', 'no'\"\n    \"Answer this about the following text, is this a happy customer?:\"\n)\n\nreviews.llm.custom(\"review\", prompt = my_prompt)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<table border=\"1\" class=\"dataframe\"><thead><tr><th>review</th><th>custom</th></tr></thead><tbody><tr><td>&quot;This has been the best TV I&#x27;ve ever used. Great screen, and sound.&quot;</td><td>&quot;Yes&quot;</td></tr><tr><td>&quot;I regret buying this laptop. It is too slow and the keyboard is too noisy&quot;</td><td>&quot;No&quot;</td></tr><tr><td>&quot;Not sure how to feel about my new washing machine. Great color, but hard to figure&quot;</td><td>&quot;No&quot;</td></tr></tbody></table></div>\n```\n\n:::\n:::\n\n\nFor more information and examples visit this function's [Python reference page](reference/MallFrame.qmd#mall.MallFrame.custom)\n:::\n\n## Model selection and settings\n\n#### Local LLMs via Ollama {#settings-local}\n\nYou can set the model and its options to use when calling the LLM. In this case,\nwe refer to options as model specific things that can be set, such as seed or \ntemperature.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nInvoking an `llm` function will automatically initialize a model selection if \nyou don't have one selected yet. If there is only one option, it will pre-select\nit for you. If there are more than one available models, then `mall` will \npresent you as menu selection so you can select which model you wish to use.\n\nCalling `llm_use()` directly will let you specify the model and backend to use.\nYou can also setup additional arguments that will be passed down to the function\nthat actually runs the prediction. In the case of Ollama, that function is [`chat()`](https://hauselin.github.io/ollama-r/reference/chat.html).\n\nThe model to use, and other options can be set for the current R session\n\n\n::: {.cell}\n\n```{.r .cell-code}\nllm_use(\"ollama\", \"llama3.2\", seed = 100, temperature = 0)\n```\n:::\n\n\n## Python\n\nThe model and options to be used will be defined at the Polars data frame object \nlevel. If not passed, the default model will be **llama3.2**.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nreviews.llm.use(\"ollama\", \"llama3.2\", options = dict(seed = 100))\n```\n:::\n\n:::\n\n#### Remote LLMs\n\nThe provider and model selection will be based on the chat object you create. \nAny model related setting, such as temperature, seed and others, should be\nset at the time of the object creation as well.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mall)\nlibrary(ellmer)\nchat <- chat_openai(model = \"gpt-4o\", seed = 100)\nllm_use(chat)\n```\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport mall\nfrom chatlas import ChatOpenAI\nchat = ChatOpenAI(model = \"gpt-4o\",  seed= 100)\ndata = mall.MallData\nreviews = data.reviews\nreviews.llm.use(chat)\n```\n:::\n\n:::\n\n\n## Results caching\n\nBy default `mall` caches the requests and corresponding results from a given \nLLM run. Each response is saved as individual JSON files. By default, the folder\nname is `_mall_cache`. The folder name can be customized, if needed. Also, the\ncaching can be turned off by setting the argument to empty (`\"\"`).\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nllm_use(.cache = \"_my_cache\")\n```\n:::\n\n\nTo turn off:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nllm_use(.cache = \"\")\n```\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nreviews.llm.use(_cache = \"my_cache\")\n```\n:::\n\n\nTo turn off:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nreviews.llm.use(_cache = \"\")\n```\n:::\n\n:::\n\nFor more information see the [Caching Results](articles/caching.qmd) article.\n\n## Key considerations\n\nThe main consideration is **cost**. Either, time cost, or money cost.\n\nIf using this method with an LLM locally available, the cost will be a long \nrunning time. Unless using a very specialized LLM, a given LLM is a general \nmodel. It was fitted using a vast amount of data. So determining a response for\neach row, takes longer than if using a manually created NLP model. The default\nmodel used in Ollama is [Llama 3.2](https://ollama.com/library/llama3.2), which \nwas fitted using 3B parameters.\n\nIf using an external LLM service, the consideration will need to be for the \nbilling costs of using such service. Keep in mind that you will be sending a \nlot of data to be evaluated.\n\nAnother consideration is the novelty of this approach. Early tests are providing\nencouraging results. But you, as an user, will still need to keep in mind that \nthe predictions will not be infallible, so always check the output. At this time,\nI think the best use for this method, is for a quick analysis.\n\n## Vector functions\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n`mall` includes functions that expect a vector, instead of a table, to run the\npredictions. This should make it easier to test things, such as custom prompts\nor results of specific text. Each `llm_` function has a corresponding `llm_vec_`\nfunction:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nllm_vec_sentiment(\"I am happy\")\n#> [1] \"positive\"\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nllm_vec_translate(\"Este es el mejor dia!\", \"english\")\n#> [1] \"It's the best day!\"\n```\n:::\n\n\n## Python \n\n`mall` is also able to process vectors contained in a `list` object. This allows\nus to avoid having to convert a list of texts without having to first convert\nthem into a single column data frame. To use, initialize a new `LLMVec` class\nobject with either an Ollama model, or a `chatlas` `Chat` object, and then\naccess the same NLP functions as the Polars extension.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Initialize a Chat object\nfrom chatlas import ChatOllama\nchat = ChatOllama(model = \"llama3.2\")\n\n# Pass it to a new LLMVec\nfrom mall import LLMVec\nllm = LLMVec(chat)    \n```\n:::\n\n\nAccess the functions via the new LLMVec object, and pass the text to be processed.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nllm.sentiment([\"I am happy\", \"I am sad\"])\n#> ['positive', 'negative']\n```\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nllm.translate([\"Este es el mejor dia!\"], \"english\")\n#> [\"It's the best day!\"]\n```\n:::\n\n\nFor more information visit the reference page: [LLMVec](reference/LLMVec.qmd)\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}