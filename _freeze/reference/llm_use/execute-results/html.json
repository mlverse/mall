{
  "hash": "34190dc90c8fa96584a28ba824106739",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Specify the model to use\"\nexecute:\n  eval: true\n  freeze: true\n---\n\n\n\n[R/llm-use.R](https://github.com/mlverse/mall/blob/main/r/R/llm-use.R)\n\n## llm_use\n\n## Description\n Allows us to specify the back-end provider, model to use during the current R session \n\n\n## Usage\n```r\n \nllm_use( \n  backend = NULL, \n  model = NULL, \n  ..., \n  .silent = FALSE, \n  .cache = NULL, \n  .force = FALSE \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| backend | The name of an supported back-end provider. Currently only 'ollama' is supported. |\n| model | The name of model supported by the back-end provider |\n| ... | Additional arguments that this function will pass down to the integrating function. In the case of Ollama, it will pass those arguments to `ollamar::chat()`. |\n| .silent | Avoids console output |\n| .cache | The path to save model results, so they can be re-used if the same operation is ran again. To turn off, set this argument to an empty character: `\"\"`. It defaults to a temp folder. If this argument is left `NULL` when calling this function, no changes to the path will be made. |\n| .force | Flag that tell the function to reset all of the settings in the R session |\n\n\n\n## Value\n A `mall_session` object \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\n \n \nlibrary(mall) \n \nllm_use(\"ollama\", \"llama3.2\") \n#> \n#> ── mall session object\n#> Backend: ollama\n#> LLM session: model:llama3.2\n#> R session:\n#> cache_folder:/var/folders/y_/f_0cx_291nl0s8h26t4jg6ch0000gp/T//Rtmpx9lgd8/_mall_cachea7cc3055324a\n \n# Additional arguments will be passed 'as-is' to the \n# downstream R function in this example, to ollama::chat() \nllm_use(\"ollama\", \"llama3.2\", seed = 100, temperature = 0.1) \n#> \n#> ── mall session object \n#> Backend: ollamaLLM session:  model:llama3.2\n#>   seed:100\n#>   temperature:0.1\n#> R session:\n#> cache_folder:/var/folders/y_/f_0cx_291nl0s8h26t4jg6ch0000gp/T//Rtmpx9lgd8/_mall_cachea7cc3055324a\n \n# During the R session, you can change any argument \n# individually and it will retain all of previous \n# arguments used \nllm_use(temperature = 0.3) \n#> \n#> ── mall session object \n#> Backend: ollamaLLM session:  model:llama3.2\n#>   seed:100\n#>   temperature:0.3\n#> R session:\n#> cache_folder:/var/folders/y_/f_0cx_291nl0s8h26t4jg6ch0000gp/T//Rtmpx9lgd8/_mall_cachea7cc3055324a\n \n# Use .cache to modify the target folder for caching \nllm_use(.cache = \"_my_cache\") \n#> \n#> ── mall session object \n#> Backend: ollamaLLM session:  model:llama3.2\n#>   seed:100\n#>   temperature:0.3\n#> R session: cache_folder:_my_cache\n \n# Leave .cache empty to turn off this functionality \nllm_use(.cache = \"\") \n#> \n#> ── mall session object \n#> Backend: ollamaLLM session:  model:llama3.2\n#>   seed:100\n#>   temperature:0.3\n \n# Use .silent to avoid the print out \nllm_use(.silent = TRUE) \n \n```\n:::\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}