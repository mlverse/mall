{
  "hash": "68e24b6eff1fc4ca4300a2de5f0d4005",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Specify the model to use\"\nexecute:\n  eval: true\n  freeze: true\n---\n\n\n\n[R/llm-use.R](https://github.com/mlverse/mall/blob/main/r/R/llm-use.R)\n\n## llm_use\n\n## Description\nAllows us to specify the back-end provider, model to use during the current R session\n\n\n## Usage\n```r\n\nllm_use(\n  backend = NULL,\n  model = NULL,\n  ...,\n  .silent = FALSE,\n  .cache = NULL,\n  .force = FALSE\n)\n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| backend | \"ollama\" or an `ellmer` `Chat` object. If using \"ollama\", `mall` will use is out-of-the-box integration with that back-end. Defaults to \"ollama\". |\n| model | The name of model supported by the back-end provider |\n| ... | Additional arguments that this function will pass down to the integrating function. In the case of Ollama, it will pass those arguments to `ollamar::chat()`. |\n| .silent | Avoids console output |\n| .cache | The path to save model results, so they can be re-used if the same operation is ran again. To turn off, set this argument to an empty character: `\"\"`. It defaults to a temp folder. If this argument is left `NULL` when calling this function, no changes to the path will be made. |\n| .force | Flag that tell the function to reset all of the settings in the R session |\n\n\n\n## Value\nA `mall_session` object\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\n\n\nlibrary(mall)\n\nllm_use(\"ollama\", \"llama3.2\")\n#> \n#> ── mall session object\n#> Backend: ollama\n#> LLM session: model:llama3.2\n#> R session:\n#> cache_folder:/var/folders/y_/f_0cx_291nl0s8h26t4jg6ch0000gp/T//RtmpLQHzPj/_mall_cache3a375fa49927\n\n# Additional arguments will be passed 'as-is' to the\n# downstream R function in this example, to ollama::chat()\nllm_use(\"ollama\", \"llama3.2\", seed = 100, temperature = 0.1)\n#> \n#> ── mall session object \n#> Backend: ollamaLLM session:  model:llama3.2\n#>   seed:100\n#>   temperature:0.1\n#> R session:\n#> cache_folder:/var/folders/y_/f_0cx_291nl0s8h26t4jg6ch0000gp/T//RtmpLQHzPj/_mall_cache3a375fa49927\n\n# During the R session, you can change any argument\n# individually and it will retain all of previous\n# arguments used\nllm_use(temperature = 0.3)\n#> \n#> ── mall session object \n#> Backend: ollamaLLM session:  model:llama3.2\n#>   seed:100\n#>   temperature:0.3\n#> R session:\n#> cache_folder:/var/folders/y_/f_0cx_291nl0s8h26t4jg6ch0000gp/T//RtmpLQHzPj/_mall_cache3a375fa49927\n\n# Use .cache to modify the target folder for caching\nllm_use(.cache = \"_my_cache\")\n#> \n#> ── mall session object \n#> Backend: ollamaLLM session:  model:llama3.2\n#>   seed:100\n#>   temperature:0.3\n#> R session: cache_folder:_my_cache\n\n# Leave .cache empty to turn off this functionality\nllm_use(.cache = \"\")\n#> \n#> ── mall session object \n#> Backend: ollamaLLM session:  model:llama3.2\n#>   seed:100\n#>   temperature:0.3\n\n# Use .silent to avoid the print out\nllm_use(.silent = TRUE)\n\n# Use an `ellmer` object\nlibrary(ellmer)\nchat <- chat_openai(model = \"gpt-4o\")\nllm_use(chat)\n#> \n#> ── mall session object \n#> Backend: ellmerLLM session:  model:gpt-4o\n#>   seed:100\n#>   temperature:0.3\n```\n:::\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}